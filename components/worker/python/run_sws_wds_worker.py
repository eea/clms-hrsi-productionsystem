import sys


import os
import pathlib
import string
import shutil
import tarfile
import traceback

from datetime import datetime
from yaml import safe_load as yaml_load
from yaml import dump as yaml_dump
import json

from typing import List


from ...common.python.database.model.job.job_status import JobStatus
from ...common.python.database.model.job.sws_wds_job import SwsWdsJob
from ...common.python.database.model.job.sws_wds_assembly_status import AssemblyStatus
from ...common.python.database.logger import Logger
from ...common.python.database.model.job.system_parameters import SystemPrameters
from ...common.python.util.sws_wds_job_util import SwsWdsJobUtil
from ...common.python.util.exceptions import CsiInternalError, CsiExternalError
from ...common.python.util.dias_storage_util import DiasStorageUtil
from ...common.python.util.sys_util import SysUtil

from .run_worker_template import RunWorker

assert sys.version_info >= (3, 0, 0)
# Check if environment variable is set, exit in error if it's not
SysUtil.ensure_env_var_set('COSIMS_DB_HTTP_API_BASE_URL')


class RunSwsWdsWorker(RunWorker):
    '''
    Class of "run_worker" object in charge of setting up a worker and run the
    dockerized scientific Snow and Ice software on it to generate SWS/WDS
    products from S1 input.

    :param root_dir: path leading to the directory in which will be stored all
        the temporary files needed and/or generated by the processing.
    '''

    def __init__(self, job, logger):

        # Call the parent constructor BEFORE all the attributes are initialized
        super().__init__(root_dir=os.environ['CSI_ROOT_DIR'])

        self.input_products = "s1"           # Name of the input folder
        self.job = job                       # Job instance of type SwsWdsJob
        self.logger = logger                 # Logger object
        self.parameters_file = None          # Path leading to the processing parameter file
        self.parameters = None               # Dictionary containing the processing parameters
        self.local_input_directories = None  # List of directories in which are stored the input used during the processing

    def assert_s1_lists_coherence(self, product_ids: list, product_eodata_paths: list):
        # Ensure both lists have the same dimension
        if len(product_ids) != len(product_eodata_paths):
            raise CsiInternalError(
                'S1 inputs inconsistency',
                f'Different number of products were set between "s1_id_list" and "s1_path_list".'
            )

        # Ensure S1 IDs and paths are arranged in the same order
        for i in range(len(product_ids)):
            if product_ids[i] not in product_eodata_paths[i]:
                raise CsiInternalError(
                    'S1 inputs inconsistency',
                    f'S1 products in the "s1_id_list" and "s1_path_list" are not'
                    ' arranged in the same order.'
                )

    def get_container_job_dir(self) -> str:
        '''
        Utility function returning the path leading to the job subdirectory in the container
        '''

        return os.path.join(
            self.get_container_work_dir(),
            self.get_job_sub_dir(self.job))

    def job_pre_processing(self):
        # TODO check if the downloaded files and directories exist as expected.

        # Update the python job object so that it is synchronized with the database,
        # because we will update some fields then apply a patch which needs an up to
        # date local version of job (in particular for some status related fields).
        jobs: List[SwsWdsJob] = SwsWdsJob().id_eq(self.job.id).get()
        self.job = jobs[0]
        self.local_input_directories = []

        self.parameters = {}
        csi_s3, csi_buckets_names = DiasStorageUtil.get_csi_buckets_conf()
        eodata_s3, eodata_bucket = DiasStorageUtil.get_eoadata_bucket_conf()
        product_s3, product_bucket = DiasStorageUtil.get_product_bucket_conf()

        ssp_aux_version = SystemPrameters().get(self.logger).ssp_aux_version
        job_dir = self.get_local_job_dir(self.job)
        tile_id = self.job.tile_id
        self.parameters['tile_id'] = tile_id
        self.parameters['fsc_paths'] = []
        self.parameters['METADATA_OF_AGGREGATED_PRODUCT'] = ""

        s1ass_path = self.job.assembly_path
        # guess my epsg
        epsg = str(32600 + int(tile_id[:2]))
        fsc_version = "V000"
        fsc_mode = "0"
        s1ass_dir = 's1_assembly/' + self.job.measurement_date.strftime("%d")
        s1_assembly_local_dir = job_dir + '/' + 's1_assembly'
        self.s1ass_local_dir = job_dir + '/' + s1ass_dir
        self.s1ass_container_dir = self.get_container_job_dir() + '/' + s1ass_dir
        self.s1ass_file = f's1_assembly_{self.job.assembly_id}.tif'
        self.generate_sws_product = self.job.assembly_params['generate_sws_product']
        self.generate_wds_product = self.job.assembly_params['generate_wds_product']

        if self.job.fsc_path_list is not None and len(self.job.fsc_path_list) > 0:
            product_local_directory = self.get_input_product_local_directory(self.job, 'fsc')
            for i in range(len(self.job.fsc_path_list)):
                product_path = self.job.fsc_path_list[i]
                product_id = self.job.fsc_id_list[i]
                # Convert
                #   "/eodata/path/to/s1/in/eodata/bucket"
                # to
                #   "path/to/s1/in/eodata/bucket"
                product_path = self.remove_prefix(product_path, '/hrsi/')
                product_path = self.append_trailing_slash_if_missing(product_path)

                fsc_product_path = os.path.join(product_local_directory, f'{product_id}')
                if os.path.isdir(fsc_product_path):
                    # Delete the product as it might be corrupted
                    shutil.rmtree(fsc_product_path)
                self.logger.info(f'create product directory ({product_local_directory})')
                pathlib.Path(product_local_directory).mkdir(parents=True, exist_ok=True)
                self.logger.info(f'get product from EODATA bucket (product path: {product_path})')
                self.download_prefixed_objects(
                    product_s3,
                    product_bucket,
                    product_path,
                    product_local_directory,
                    logger=self.logger)
                self.parameters['fsc_paths'].append(f'/work/jobs/{self.job.unique_id}/fsc/{product_id}/{product_id}_FSCTOC.tif')
                with open(f"{fsc_product_path}/{product_id}_MTD.xml") as fd:
                    fsc_mtd = "".join(fd.readlines()[1:])
                self.parameters['METADATA_OF_AGGREGATED_PRODUCT'] += fsc_mtd
            self.local_input_directories.append(product_local_directory)
            fsc_version, fsc_mode = product_id.split('_')[-2:]

        # ---------- Get the S1 product ----------
        # Retrieve inputs S1 IDs and paths
        if self.job.s1_path_list is not None and len(self.job.s1_path_list) > 0 and s1ass_path is None:
            product_paths = self.job.s1_path_list
            product_ids = self.job.s1_id_list
            self.parameters['s1_paths'] = [f"/work/jobs/{self.job.unique_id}/s1/{product_id}.SAFE" for product_id in product_ids]

            # Ensure that list of S1 IDs and paths are coherent
            self.assert_s1_lists_coherence(product_ids, product_paths)

            product_local_directory = self.get_input_product_local_directory(self.job, self.input_products)

            # List of directories  where we donwload some inputs and that we don't need
            # to store in the results bucket at the end of the processing.

            # Get the input product and put it in the expected input directory for S&I
            # processing softwares.
            for i in range(len(product_paths)):
                # Convert
                #   "/eodata/path/to/s1/in/eodata/bucket"
                # to
                #   "path/to/s1/in/eodata/bucket"
                product_path = self.remove_prefix(product_paths[i], '/eodata/')
                product_path = self.append_trailing_slash_if_missing(product_path)

                s1_product_path = os.path.join(product_local_directory, f'{product_ids[i]}.SAFE')
                if os.path.exists(s1_product_path):
                    # Delete the product as it might be corrupted
                    shutil.rmtree(s1_product_path)
                self.logger.info(f'create product directory ({product_local_directory})')
                pathlib.Path(product_local_directory).mkdir(parents=True, exist_ok=True)
                self.logger.info(f'get product from EODATA bucket (product path: {product_path})')
                self.download_prefixed_objects(
                    eodata_s3,
                    eodata_bucket,
                    product_path,
                    product_local_directory,
                    logger=self.logger)

            self.local_input_directories.append(product_local_directory)
            self.parameters['s1ass_create'] = True

            # ---------- Get the DEM file ----------
            # TODO: set path for the DEM
            self.logger.info(f'get DEM file')
            aux_local_file_path = self.compute_local_file_path(
                f'GLO30/DEM_60m_zones_wgs84/Copernicus_DSM_60m_{epsg}_wgs84.tif',
                f'{job_dir}/aux',
                logger=self.logger)
            self.download_file_from_bucket(
                csi_s3, csi_buckets_names['sip_aux'],
                f'GLO30/DEM_60m_zones_wgs84/Copernicus_DSM_60m_{epsg}_wgs84.tif',
                aux_local_file_path)
        else:
            self.logger.info(f'download input S1 Assembly file for this job from {s1ass_path}')
            self.parameters['s1ass_create'] = False
            self.parameters['s1_paths'] = ""

            aux_local_file_path = os.path.join(self.s1ass_local_dir, self.s1ass_file)
            os.makedirs(os.path.dirname(aux_local_file_path), exist_ok=True)
            # FIXME: hidden_value not in csi_buckets_names of DiasStorageUtil.get_csi_buckets_conf
            (bucket, object_path) = SwsWdsJobUtil.split_bucket_and_object_from_path(s1ass_path)
            self.download_file_from_bucket(
                csi_s3, bucket,
                object_path,
                aux_local_file_path)

        # ---------- Get the aux files ----------
        self.logger.info(f'get common auxiliary files')
        object_path = f'ssp_aux/{ssp_aux_version}/ssp_aux_common.tar'
        aux_local_file_path = self.compute_local_file_path(
            object_path,
            f'{job_dir}/aux',
            logger=self.logger)
        self.download_file_from_bucket(
            csi_s3, csi_buckets_names['sip_aux'],
            object_path,
            aux_local_file_path)
        with tarfile.open(f'{aux_local_file_path}') as tar:
            tar.extractall(f'{job_dir}/aux')

        track = "%03d" % self.job.assembly_params['relativeOrbitNumber']
        month = self.job.measurement_date.strftime('%m')
        self.logger.info(f'get auxiliary files for track {track}')
        object_path = f'ssp_aux/{ssp_aux_version}/{tile_id}/T{tile_id}_60m_t{track}_S1_AUX_{ssp_aux_version}.tar'
        aux_local_file_path = self.compute_local_file_path(
            object_path,
            f'{job_dir}/aux',
            logger=self.logger)
        self.download_file_from_bucket(
            csi_s3, csi_buckets_names['sip_aux'],
            object_path,
            aux_local_file_path)
        with tarfile.open(f'{aux_local_file_path}') as tar:
            tar.extractall(f'{job_dir}/aux')

        self.logger.info(f'get FOREST_URBAN_WATER mask file')
        object_path = f'ssp_aux/{ssp_aux_version}/{tile_id}/T{tile_id}_60m_MASK_FOREST_URBAN_WATER_{ssp_aux_version}.tif'
        aux_local_file_path = self.compute_local_file_path(
            object_path,
            f'{job_dir}/aux',
            logger=self.logger)
        self.download_file_from_bucket(
            csi_s3, csi_buckets_names['sip_aux'],
            object_path,
            aux_local_file_path)

        if self.generate_sws_product:
            self.logger.info(f'get NON_MOUNTAIN_AREA mask file')
            object_path = f'ssp_aux/{ssp_aux_version}/{tile_id}/T{tile_id}_60m_MASK_NON_MOUNTAIN_AREA_{ssp_aux_version}.tif'
            aux_local_file_path = self.compute_local_file_path(
                object_path,
                f'{job_dir}/aux',
                logger=self.logger)
            self.download_file_from_bucket(
                csi_s3, csi_buckets_names['sip_aux'],
                object_path,
                aux_local_file_path)
            self.logger.info(f'get SNOW mask file for month {month}')
            object_path = f'ssp_aux/{ssp_aux_version}/{tile_id}/T{tile_id}_60m_MASK_SNOW_m{month}_{ssp_aux_version}.tif'
            aux_local_file_path = self.compute_local_file_path(
                object_path,
                f'{job_dir}/aux',
                logger=self.logger)
            self.download_file_from_bucket(
                csi_s3, csi_buckets_names['sip_aux'],
                object_path,
                aux_local_file_path)

        self.local_input_directories.append(f'{job_dir}/aux')
        self.local_input_directories.append(s1_assembly_local_dir)

        # ---------- Parameters update & parameters file creation ----------
        self.parameters['ssp_aux_version'] = ssp_aux_version
        self.parameters['track'] = track
        self.parameters['month'] = month
        self.parameters['measurement_date'] = self.job.measurement_date.strftime('%Y%m%dT%H%M%S')
        self.parameters['s1ass_file'] = self.s1ass_file
        self.parameters['s1ass_dir'] = self.s1ass_container_dir
        self.parameters['job_dir'] = self.get_container_job_dir()
        self.parameters['aux_dir'] = os.path.join(self.parameters['job_dir'], 'aux')
        self.parameters['out_dir'] = os.path.join(self.parameters['job_dir'], 'outputs')
        self.parameters['platform'] = self.job.assembly_params['platform'].upper()
        self.parameters['missionTakeId'] = self.job.assembly_params['missionTakeId']
        self.parameters['epsg'] = epsg
        self.parameters['sws_product_id'] = f"SWS_{self.parameters['measurement_date']}_{self.parameters['platform']}_T{tile_id}_V101_1"
        self.parameters['wds_product_id'] = f"WDS_{self.parameters['measurement_date']}_{self.parameters['platform']}_T{tile_id}_V101_{fsc_mode}"
        # self.parameters['wds_product_id'] = f"WDS_{self.parameters['measurement_date']}_{self.parameters['platform']}_T{tile_id}_{fsc_version}_{fsc_mode}"
        self.parameters['assembly_id'] = self.job.assembly_id
        self.parameters['generate_sws_product'] = self.generate_sws_product
        self.parameters['generate_wds_product'] = self.generate_wds_product

        tile_envelope = self.job.assembly_params['envelope'].split(';')                     # (minX, minY, maxX, maxY)
        self.parameters['WB_LON'] = tile_envelope[0]
        self.parameters['SB_LAT'] = tile_envelope[1]
        self.parameters['EB_LON'] = tile_envelope[2]
        self.parameters['NB_LAT'] = tile_envelope[3]
        self.parameters['ACQUISITION_START'] = self.job.measurement_date.isoformat()
        self.parameters['ACQUISITION_STOP'] = self.job.assembly_params['sourceProduct_stopTime']

        self.parameters['job_unique_id'] = self.job.unique_id

        self.logger.info(f'create the S&I processing parameters file')
        self.create_parameters_file()

    def create_parameters_file(self):
        # Insert some values from the parameters dictionary in a template for the
        # S&I processing parameter file.
        parameters_template_file = os.path.join(self.get_local_job_dir(self.job), 'aux', 'sws_wds_processing_parameters_template.yaml')
        parameters_template = pathlib.Path(parameters_template_file).read_text()
        parameters_string = string.Template(parameters_template).substitute(self.parameters)
        self.parameters_file = os.path.join(
            'jobs', str(self.job.unique_id), 'processings_parameters.yaml')
        pf = os.path.join(self.get_work_dir(), self.parameters_file)
        pathlib.Path(pf).write_text(parameters_string)
        with open(pf) as fd:
            self.parameters_yaml = yaml_load(fd)

    def create_mtd_file(self, product, outfile):
        # Insert some values from the parameters dictionary in a template for the
        # Product Metadata XML file
        # TODO: get and insert the Metadata of the Fsc Product into the WDS Product Metadata
        parameters_template_file = os.path.join(self.get_local_job_dir(self.job), "aux", f"{product}_Metadata.xml")
        parameters_template = pathlib.Path(parameters_template_file).read_text()
        parameters_string = string.Template(parameters_template).substitute(self.parameters)
        pf = os.path.join(self.get_local_job_dir(self.job), 'outputs', 'data', self.parameters['PRODUCT_ID'], outfile)
        pathlib.Path(pf).write_text(parameters_string)

    def job_processing(self):
        # TODO: create the SystemParameter for the docker image name
        docker_image = SystemPrameters().get(self.logger.debug).docker_image_for_sws_wds_processing
        container_parameter_file_path = os.path.join(self.get_container_work_dir(), self.parameters_file)
        processing_exe_name = '/install/ssp/bin/sws_wds_create_wsm.py'
        container_command = f'{processing_exe_name} "{container_parameter_file_path}"'
        work_dir = self.get_work_dir()

        processing_status = self.run_processing_in_docker(
            image_name=docker_image,
            container_name=f'sws_wds_processing_{self.job.id}',
            command=container_command,
            volumes_binding={
                f'{work_dir}': {  # The local working dir
                    'bind': self.get_container_work_dir(),  # the dir in the container, something like '/work'
                    'mode': 'rw'
                },
            },
            logger=self.logger
        )

        # TODO: store the image name and result
        self.job.sws_wds_processing_image = docker_image
        # FIXME: where to store processing_status
        # self.job.si_processing_return_code = processing_status
        self.job.patch(patch_foreign=True, logger_func=self.logger.debug)

        # Check the status returned by the processing
        self.check_processing_status(processing_status)

    def check_processing_status(self, si_processing_status):

        if si_processing_status >= 0xD0 and si_processing_status <= 0xD7:
            if si_processing_status & 0x01:
                self.parameters['s1ass_create'] = False
                self.job.assembly_status = AssemblyStatus.empty.name
            if si_processing_status & 0x02:
                self.generate_sws_product = False
            if si_processing_status & 0x04:
                self.generate_wds_product = False

        if si_processing_status == 0:
            self.logger.info(f'processing finished with no error, proceed')

        elif si_processing_status < 100:
            message = (
                f'the WS processing finished with an unrecoverable error (status '
                f'code = {si_processing_status})'
            )
            raise CsiInternalError('SIP unrecoverable error', message)

        elif si_processing_status < 200:
            message = (
                f'the WS processing finished with an expected error (status code = '
                f'{si_processing_status}), we can try to run processing again'
            )
            raise CsiExternalError('SIP expected error', message)

        elif si_processing_status < 250:
            message = (
                f'the WS processing finished with no error, but (status '
                f'code = {si_processing_status})'
            )
            self.logger.info(f'processing finished with no error (status code = {si_processing_status}, proceed')

        else:
            message = (
                f'the WS processing finished with unknown error (status '
                f'code = {si_processing_status})'
            )
            raise CsiInternalError('SIP expected error', message)

    def manage_results(self, completion_date: str):

        csi_s3, csi_buckets_names = DiasStorageUtil.get_csi_buckets_conf()
        csi_products_s3, csi_products_bucket = DiasStorageUtil.get_product_bucket_conf()

        measurement_path = self.job.measurement_date.strftime("%Y/%m/%d")
        self.parameters['PRODUCTION_DATE'] = completion_date.isoformat()
        self.parameters['EDITION_DATE'] = completion_date.isoformat()

        if self.parameters['s1ass_create']:
            self.logger.info(f'put the S1 Assembly in the CoSIMS object storage')
            product_out = f"hidden_value/s1_assembly/{self.job.assembly_params['platform']}/{measurement_path}/{self.s1ass_file}"
            (bucket, object_path) = SwsWdsJobUtil.split_bucket_and_object_from_path(product_out)
            self.upload_file(
                csi_s3,
                bucket,
                os.path.join(self.s1ass_local_dir, self.s1ass_file),
                object_path
            )
            self.job.assembly_path = product_out
            self.job.assembly_status = AssemblyStatus.generated.name

        if self.generate_sws_product:
            product_id = self.parameters['sws_product_id']

            # create INSPIRE XML file
            self.parameters['PRODUCT_ID'] = product_id
            self.create_mtd_file("SWS", product_id + "_MTD.xml")

            self.logger.info(f'upload thumbnail of SWS product')
            product_out = f"HRSI/Preview/CLMS/Pan-European/High_Resolution_Layers/Snow/SWS/{measurement_path}/{product_id}/thumbnail.png"
            result_local = self.get_local_results_dir(self.job) + f"/data/{product_id}/thumbnail.png"
            (bucket, object_path) = SwsWdsJobUtil.split_bucket_and_object_from_path(product_out)
            self.upload_file(
                csi_products_s3,
                csi_products_bucket,
                result_local,
                object_path
            )
            os.remove(result_local)

            self.logger.info(f'upload directory of SWS product')
            product_out = f"HRSI/CLMS/Pan-European/High_Resolution_Layers/Snow/SWS/{measurement_path}"
            result_local = self.get_local_results_dir(self.job) + '/data/' + product_id
            (bucket, object_path) = SwsWdsJobUtil.split_bucket_and_object_from_path(product_out)
            self.upload_directory(
                csi_products_s3,
                csi_products_bucket,
                result_local,
                object_path
            )
            self.job.sws_path = "/eodata/" + product_out + "/" + product_id
            resourceSize = os.path.getsize(os.path.join(result_local, os.path.basename(self.parameters_yaml['wsm'])))
            resourceSize += os.path.getsize(os.path.join(result_local, os.path.basename(self.parameters_yaml['qcwsm'])))
            resourceSize += os.path.getsize(os.path.join(result_local, product_id + "_MTD.xml"))
            self.job.sws_infos["resto"]["properties"] = {"resourceSize": resourceSize / 1024, "resolution": self.parameters_yaml['pixelsize']}
            self.job.sws_completion_date = completion_date

        if self.generate_wds_product:
            product_id = self.parameters['wds_product_id']

            # create INSPIRE XML file
            self.parameters['PRODUCT_ID'] = product_id
            self.create_mtd_file("WDS", product_id + "_MTD.xml")

            self.logger.info(f'upload thumbnail of WDS product')
            product_out = f"HRSI/Preview/CLMS/Pan-European/High_Resolution_Layers/Snow/WDS/{measurement_path}/{product_id}/thumbnail.png"
            result_local = self.get_local_results_dir(self.job) + f"/data/{product_id}/thumbnail.png"
            (bucket, object_path) = SwsWdsJobUtil.split_bucket_and_object_from_path(product_out)
            self.upload_file(
                csi_products_s3,
                csi_products_bucket,
                result_local,
                object_path
            )
            os.remove(result_local)

            self.logger.info(f'upload directory of WDS product')
            product_out = f"HRSI/CLMS/Pan-European/High_Resolution_Layers/Snow/WDS/{measurement_path}"
            result_local = self.get_local_results_dir(self.job) + '/data/' + product_id
            (bucket, object_path) = SwsWdsJobUtil.split_bucket_and_object_from_path(product_out)
            self.upload_directory(
                csi_products_s3,
                csi_products_bucket,
                result_local,
                object_path
            )
            self.job.wds_path = "/eodata/" + product_out + "/" + product_id
            resourceSize = os.path.getsize(os.path.join(result_local, os.path.basename(self.parameters_yaml['ssc'])))
            resourceSize += os.path.getsize(os.path.join(result_local, os.path.basename(self.parameters_yaml['qcssc'])))
            resourceSize += os.path.getsize(os.path.join(result_local, product_id + "_MTD.xml"))
            self.job.wds_infos["resto"]["properties"] = {"resourceSize": resourceSize / 1024, "resolution": self.parameters_yaml['pixelsize']}
            self.job.wds_completion_date = completion_date

        # cleanup job directory
        local_results_dir = self.get_local_results_dir(self.job)
        self.logger.info(f'  remove {local_results_dir}')
        shutil.rmtree(local_results_dir)

    def job_post_processing(self):

        completion_date = datetime.now()

        self.manage_results(completion_date)
        self.logger.info(f'update some job informations')
        self.job.patch(patch_foreign=True, logger_func=self.logger.debug)


# Function does the minimal stuff to get a valid job, an associate logger and
# launches the 'run' function which actually executes the tasks. This function
# catch errors from 'run' command and "sends" them to the database (with the
# status change).
def main(job_id: int):
    jobs: List[SwsWdsJob] = SwsWdsJob().id_eq(job_id).get()

    if len(jobs) == 0:
        # TODO how can we notify this very critical internal error? As we
        # found no job, this message will only appear on the stdout of the
        # Nomad job. Find a way to manage this.
        print(f'there is no job which ID is {job_id}')
        exit(1)

    job = jobs[0]
    logger = Logger(job.post_new_execution_info())

    # Create an instance of the RunWorker class to process the job.
    run_worker_instance = RunSwsWdsWorker(job, logger)

    try:
        run_worker_instance.run()
        logger.info('run finished')
        error = None
    except Exception as exception:
        error = exception
        if not isinstance(error, (CsiInternalError, CsiExternalError)):
            logger.error(traceback.format_exc())
    finally:
        run_worker_instance.error_management_and_exit(
            error
        )
