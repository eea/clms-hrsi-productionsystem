import sys

assert sys.version_info >= (3, 0, 0)

import os
import pathlib
import subprocess
import string
import shutil
from datetime import datetime
import time
import socket
import tarfile
import json
import yaml
import xmltodict
import traceback

from typing import List

# Check if environment variable is set, exit in error if it's not
from ...common.python.util.sys_util import SysUtil
SysUtil.ensure_env_var_set('COSIMS_DB_HTTP_API_BASE_URL')

from ...common.python.database.model.job.gfsc_job import GfscJob
from ...common.python.database.model.job.job_status import JobStatus
from ...common.python.database.logger import Logger
from ...common.python.database.model.job.system_parameters import SystemPrameters
from ...common.python.util.gfsc_job_util import GfscJobUtil
from ...common.python.util.exceptions import CsiInternalError, CsiExternalError
from ...common.python.util.dias_storage_util import DiasStorageUtil

from .run_worker_template import RunWorker



class RunGfscWorker(RunWorker):
    '''
    Class of "run_worker" object in charge of setting up a worker and run the
    dockerized scientific Snow and Ice software on it to generate GFSC
    products from FSC, WDS, SWS and GFSC product(s).

    :param root_dir: path leading to the directory in which will be stored all
        the temporary files needed and/or generated by the processing.
    '''

    def __init__(self, job, logger):

        # Call the parent constructor BEFORE all the attributes are initialized
        super().__init__(root_dir=os.environ['CSI_ROOT_DIR'])

        self.logger = logger                # Logger object
        self.job = job                      # Job instance of type GfscJob

        # Start of the specific members
        self.local_input_directories = None # List of directories in which are stored the input used during the processing
        self.parameters_file = None         # Path leading to the processing parameter file
        self.parameters = None              # Dictionary containing the processing parameters
        self.results_are_available = None   # Boolean notifying if products have been generated by the processing
        # End of the specific members


# Start of specific methods
    def up_one_directory(self, path: str):
        """Move file in path up one directory"""

        # Move the sub-elements at their parent directory level
        for sub_element in os.listdir(path):
            p = pathlib.Path(os.path.join(path, sub_element)).absolute()
            parent_dir = p.parents[1]

            # Skip the folder re-organisation, as the sub-folder to move is empty,
            # and it already exists in the destination folder, and is not empty.
            if (
                not os.listdir(os.path.join(path, sub_element))
                and os.path.exists(parent_dir / p.name)
                and os.listdir(parent_dir / p.name)
            ):
                self.logger.warning(f'Skipping sub-folder {os.path.join(path, sub_element)} '\
                    'move upward as it s empty  and the folder already exists in the destination !')
                break

            # Log a warning message if the subfolder to move is empty
            elif not os.listdir(os.path.join(path, sub_element)):
                self.logger.warning(f'The sub-folder {os.path.join(path, sub_element)} '\
                    'which is planned to be moved upward is empty !')

            # If the sub-folder to move is not empty and a folder already exists in the
            # destination with the same name, we remove this one before moving the sub-folder.
            elif os.path.exists(parent_dir / p.name) and os.listdir(os.path.join(path, sub_element)):
                shutil.rmtree(parent_dir / p.name)

            p.rename(parent_dir / p.name)

        # Remove the original sub-elements parent directory
        shutil.rmtree(path)
# End of specific methods


# Start of mandatory methods
    def job_pre_processing(self):
        # Update the python job object so that it is synchronized with the database,
        # because we will update some fields then apply a patch which needs an up to
        # date local version of job (in particular for some status related fields).

        jobs: List[GfscJob] = GfscJob().id_eq(self.job.id).get()
        self.job = jobs[0]

        self.parameters = {}

        csi_s3, csi_buckets_names = DiasStorageUtil.get_csi_buckets_conf()
        csi_products_s3, csi_products_bucket = DiasStorageUtil.get_product_bucket_conf()

        job_dir = self.get_local_job_dir(self.job)
        tile_id = self.job.tile_id
        self.parameters['tile_id'] = tile_id

        # List of directories  where we download some inputs and that we don't need
        # to store in the results bucket at the end of the processing.
        self.local_input_directories = []


        # ---------- Get the input products ----------
        # TODO [Minor] factorized code to take product type as param (fsc, wds, sws, gfsc).
        input_product_local_directory = self.get_input_product_local_directory(self.job, "gfsc")
        # Remove folder if it exists
        input_product_local_directory_path = pathlib.Path(input_product_local_directory)
        if input_product_local_directory_path.exists and input_product_local_directory_path.is_dir():
            shutil.rmtree(input_product_local_directory_path)
        os.makedirs(input_product_local_directory)
        self.local_input_directories.append(input_product_local_directory)
        self.parameters['gfsc_dir'] = self.remove_prefix(input_product_local_directory,self.root_dir)
        self.parameters['gfsc_id_list'] = []
        self.logger.info('Fetching input products (GFSC)')
        for i,input_product_id in enumerate(self.job.gfsc_id_list):
            self.parameters['gfsc_id_list'].append(input_product_id)
            # Get the input product and put it in the expected input directory for S&I
            # processing softwares.
            product_bucket_path = GfscJobUtil.get_product_bucket_path(input_product_id)
            input_product_path = os.path.join(input_product_local_directory, input_product_id)
            if not os.path.isdir(input_product_path):
                self.logger.info(f'create product directory ({input_product_local_directory})')
                pathlib.Path(input_product_local_directory).mkdir(parents=True, exist_ok=True)
                self.logger.info(f'get product from the HR-S&I products bucket (product path: {product_bucket_path})')
                self.download_prefixed_objects(
                    csi_products_s3,
                    csi_products_bucket,
                    product_bucket_path,
                    input_product_local_directory,
                    logger=self.logger)

        # ---------- Find obsolete products (which are included in GFSC products) ----------
        self.logger.info('Identifying obsolete input products.')
        self.find_obsolete_input_products()
        self.logger.info('%i obsolete products found.' % len(self.job.obsolete_product_id_list))

        # ---------- Get the input products ----------
        input_product_local_directory = self.get_input_product_local_directory(self.job, "fsc")
        # Remove input folder if it exists
        input_product_local_directory_path = pathlib.Path(input_product_local_directory)
        if input_product_local_directory_path.exists and input_product_local_directory_path.is_dir():
            shutil.rmtree(input_product_local_directory_path)
        os.makedirs(input_product_local_directory)
        self.local_input_directories.append(input_product_local_directory)
        self.parameters['obsolete_product_id_list'] = []
        self.parameters['fsc_dir'] = self.remove_prefix(input_product_local_directory,self.root_dir)
        self.parameters['fsc_id_list'] = []
        self.logger.info('Fetching input products (FSC)')
        for i,input_product_id in enumerate(self.job.fsc_id_list):
            if input_product_id in self.job.obsolete_product_id_list:
                self.parameters['obsolete_product_id_list'].append(input_product_id)
                continue
            self.parameters['fsc_id_list'].append(input_product_id)
            # Get the input product and put it in the expected input directory for S&I
            # processing softwares.
            product_bucket_path = GfscJobUtil.get_product_bucket_path(input_product_id)
            input_product_path = os.path.join(input_product_local_directory, input_product_id)
            if not os.path.isdir(input_product_path):
                self.logger.info(f'create product directory ({input_product_local_directory})')
                pathlib.Path(input_product_local_directory).mkdir(parents=True, exist_ok=True)
                self.logger.info(f'get product from the HR-S&I products bucket (product path: {product_bucket_path})')
                self.download_prefixed_objects(
                    csi_products_s3,
                    csi_products_bucket,
                    product_bucket_path,
                    input_product_local_directory,
                    logger=self.logger)

        input_product_local_directory = self.get_input_product_local_directory(self.job, "wds")
        # Remove input folder if it exists
        input_product_local_directory_path = pathlib.Path(input_product_local_directory)
        if input_product_local_directory_path.exists and input_product_local_directory_path.is_dir():
            shutil.rmtree(input_product_local_directory_path)
        os.makedirs(input_product_local_directory)
        self.local_input_directories.append(input_product_local_directory)
        self.parameters['wds_dir'] = self.remove_prefix(input_product_local_directory,self.root_dir)
        self.parameters['wds_id_list'] = []
        self.logger.info('Fetching input products (WDS)')
        for i,input_product_id in enumerate(self.job.wds_id_list):
            if input_product_id in self.job.obsolete_product_id_list:
                self.parameters['obsolete_product_id_list'].append(input_product_id)
                continue
            self.parameters['wds_id_list'].append(input_product_id)
            # Get the input product and put it in the expected input directory for S&I
            # processing softwares.
            product_bucket_path = GfscJobUtil.get_product_bucket_path(input_product_id)
            input_product_path = os.path.join(input_product_local_directory, input_product_id)
            if not os.path.isdir(input_product_path):
                self.logger.info(f'create product directory ({input_product_local_directory})')
                pathlib.Path(input_product_local_directory).mkdir(parents=True, exist_ok=True)
                self.logger.info(f'get product from the HR-S&I products bucket (product path: {product_bucket_path})')
                self.download_prefixed_objects(
                    csi_products_s3,
                    csi_products_bucket,
                    product_bucket_path,
                    input_product_local_directory,
                    logger=self.logger)

        input_product_local_directory = self.get_input_product_local_directory(self.job, "sws")
        # Remove input folder if it exists
        input_product_local_directory_path = pathlib.Path(input_product_local_directory)
        if input_product_local_directory_path.exists and input_product_local_directory_path.is_dir():
            shutil.rmtree(input_product_local_directory_path)
        os.makedirs(input_product_local_directory)
        self.local_input_directories.append(input_product_local_directory)
        self.parameters['sws_dir'] = self.remove_prefix(input_product_local_directory,self.root_dir)
        self.parameters['sws_id_list'] = []
        self.logger.info('Fetching input products (SWS)')
        for i,input_product_id in enumerate(self.job.sws_id_list):
            if input_product_id in self.job.obsolete_product_id_list:
                self.parameters['obsolete_product_id_list'].append(input_product_id)
                continue
            self.parameters['sws_id_list'].append(input_product_id)
            # Get the input product and put it in the expected input directory for S&I
            # processing softwares.
            product_bucket_path = GfscJobUtil.get_product_bucket_path(input_product_id)
            input_product_path = os.path.join(input_product_local_directory, input_product_id)
            if not os.path.isdir(input_product_path):
                self.logger.info(f'create product directory ({input_product_local_directory})')
                pathlib.Path(input_product_local_directory).mkdir(parents=True, exist_ok=True)
                self.logger.info(f'get product from the HR-S&I products bucket (product path: {product_bucket_path})')
                self.download_prefixed_objects(
                    csi_products_s3,
                    csi_products_bucket,
                    product_bucket_path,
                    input_product_local_directory,
                    logger=self.logger)

        # ---------- Get the aux files ----------
        # Add the AUX to the list of folders to clean
        ssp_aux_version = SystemPrameters().get(self.logger).ssp_aux_version
        aux_local_directory = f'{job_dir}/aux/{tile_id[1:]}'
        self.local_input_directories.append(f'{job_dir}/aux')
        pathlib.Path(aux_local_directory).mkdir(parents=True, exist_ok=True)
        self.parameters['aux_dir'] = self.remove_prefix(aux_local_directory,self.root_dir)

        self.logger.info(f'get auxiliary files (static masks)')

        fuw_mask_file = f'{tile_id}_60m_MASK_FOREST_URBAN_WATER_{ssp_aux_version}.tif'
        self.download_file_from_bucket(
            csi_s3, csi_buckets_names['sip_aux'],
            f'ssp_aux/{ssp_aux_version}/{tile_id[1:]}/{fuw_mask_file}',
            f'{aux_local_directory}/{fuw_mask_file}')
        self.parameters['fuw_mask_file'] = fuw_mask_file

        nm_mask_file = f'{tile_id}_60m_MASK_NON_MOUNTAIN_AREA_{ssp_aux_version}.tif'
        if self.file_exists_in_bucket(
            csi_s3, csi_buckets_names['sip_aux'],
            f'ssp_aux/{ssp_aux_version}/{tile_id[1:]}/{nm_mask_file}'):
            self.download_file_from_bucket(
                csi_s3, csi_buckets_names['sip_aux'],
                f'ssp_aux/{ssp_aux_version}/{tile_id[1:]}/{nm_mask_file}',
                f'{aux_local_directory}/{nm_mask_file}')
        self.parameters['nm_mask_file'] = nm_mask_file

        self.logger.info(f'get auxiliary files (DEM)')
        self.download_file_from_bucket(
            csi_s3, csi_buckets_names['sip_aux'],
            f'GLO30/DEM_60m_tiles_wgs84/Copernicus_DSM_60m_{tile_id[1:]}_wgs84.tif',
            os.path.join(aux_local_directory,f'Copernicus_DSM_60m_{tile_id[1:]}_wgs84.tif'))
        self.parameters['dem_file'] = f'Copernicus_DSM_60m_{tile_id[1:]}_wgs84.tif'

        # self.local_input_directories.append(f'{job_dir}/{tile_id[1:]}')
        # ---------- Parameters update & sip parameters file creation ----------
        self.parameters['job_unique_id'] = self.job.unique_id
        self.parameters['aggregation_timespan'] = self.job.aggregation_timespan
        self.parameters['product_title'] = self.job.gfsc_id
        self.parameters['output_dir'] = self.remove_prefix(self.get_local_results_dir(self.job),self.root_dir)
        self.parameters['tmp_dir'] = self.remove_prefix(os.path.join(self.get_local_job_dir(self.job),'tmp'),self.root_dir)
        self.local_input_directories.append(os.path.join(self.get_local_job_dir(self.job),'tmp'))
        self.parameters['work_dir'] = self.remove_prefix(job_dir,self.root_dir)

        self.logger.info(f'create the S&I processing parameters file')
        self.create_sip_parameters_file()

    def find_obsolete_input_products(self):
        '''
        Finds input products which all input products in its sensing date 
        is already included in the input GFSC. All input products in the 
        same date should exists in GFSC, otherwise daily spatial gap filling
        should be done again.
        Function is implemented as gfsc_id_list only has one member (as it is
        assigned in GfscJob.configure_single_job)
        '''

        if self.job.obsolete_product_id_list is None:
          self.job.obsolete_product_id_list = []

        if self.job.gfsc_id_list is None or self.job.gfsc_id_list == [] or self.job.gfsc_id_list == '':
            return

        xml_file = os.path.join(self.get_input_product_local_directory(self.job, "gfsc"), self.job.gfsc_id_list[0],self.job.gfsc_id_list[0]
        +'_MTD.xml')
        xml_dict = xmltodict.parse(open(xml_file,'r').read())
        input_product_xmls = xml_dict['gmd:MD_Metadata']['gmd:series']['gmd:DS_OtherAggregate']['gmd:seriesMetadata']
        
        gfsc_input_list = []
        if len(input_product_xmls) == 1:
            input_product_xmls = [input_product_xmls]
        for input_product_xml in input_product_xmls:
            gfsc_input_list.append(input_product_xml['gmd:MD_Metadata']['gmd:fileIdentifier']['gco:CharacterString'])

        input_list = self.job.fsc_id_list + self.job.wds_id_list + self.job.sws_id_list
        product_dates = [GfscJobUtil.get_input_product_date(product_id) for product_id in input_list]
        product_dates = list(set(product_dates))
        for product_date in product_dates:
            input_today_list = [product_id for product_id in input_list if GfscJobUtil.get_input_product_date(product_id) == product_date]
            if all([product_id in gfsc_input_list for product_id in input_today_list]):
                for product_id in input_today_list:
                    self.job.obsolete_product_id_list.append(product_id)

    def create_sip_parameters_file(self):
        self.parameters_file = os.path.join(
            'jobs', str(self.job.unique_id), 'processings_parameters.yaml')
        f = open(os.path.join(self.get_work_dir(), self.parameters_file),'w')
        f.write(yaml.dump(self.parameters))
        f.close()

    def job_processing(self):
        docker_image = SystemPrameters().get(self.logger.debug).docker_image_for_gfsc_processing
        processing_exe_name = 'python3 gf.py'
        container_parameter_file_path = os.path.join(self.get_container_work_dir(), self.parameters_file)
        container_command = f'{processing_exe_name} {container_parameter_file_path}'
        work_dir = self.get_work_dir()

        si_processing_status = self.run_processing_in_docker(
            image_name=docker_image,
            container_name=f'gfsc_processing_{self.job.id}',
            command=container_command,
            volumes_binding={
                f'{work_dir}': { # The local working dir
                    'bind': self.get_container_work_dir(), # the dir in the container, something like '/work'
                    'mode': 'rw'
                    }
                },
            logger=self.logger
            )


        self.job.si_processing_image = docker_image
        self.job.patch(patch_foreign=True, logger_func=self.logger.debug)

        # Check the status returned by the processing
        self.check_si_processing_status(si_processing_status)

    def check_si_processing_status(self, si_processing_status):
        #TODO [Minor] add docstring
        #TODO [Major] check return codes
        self.results_are_available = False
        if si_processing_status == 0:
            self.logger.info(f'processing finished with no error, proceed')
            self.results_are_available = True
        elif si_processing_status < 100:
            message = (
                f'the S&I processing finished with an unrecoverable error (status '
                f'code = {si_processing_status})'
            )
            raise CsiInternalError('SIP unrecoverable error', message)
        elif si_processing_status < 200:
            message = (
                f'the S&I processing finished with an expected error (status code = '
                f'{si_processing_status}), we can try to run processing again'
            )
            raise CsiExternalError('SIP expected error', message)


    def job_post_processing(self):

        csi_products_s3, csi_products_bucket = DiasStorageUtil.get_product_bucket_conf()

        local_results_dir = self.get_local_results_dir(self.job)
        with open(f'{local_results_dir}/data/output_info.yaml', 'r') as file:
            output_info = yaml.safe_load(file)

        # Add GFSC1 products to local directories to be removed
        # GFSC1 products are saved in output data directory, not in input
        for gfsc_dir in os.listdir(os.path.join(local_results_dir,'data')):
            if 'GFSC1_' in gfsc_dir:
                self.local_input_directories.append(os.path.join(local_results_dir,'data',gfsc_dir))

        # TODO [Major] ensure that the gfsc_id is not set twice with different values
        self.job.gfsc_id = output_info['product_title']
        outputs_directory = os.path.join(
            local_results_dir, 'data', self.job.gfsc_id)

        self.logger.info(f'set the bucket path for the GFSC product')
        GfscJobUtil.set_product_bucket_paths(
            self.job,
            "GFSC",
            self.job.gfsc_id,
            logger_func=self.logger.debug
        )

        self.logger.info(f'get some infos about the GFSC results from JSON file')
        gfsc_infos_file_name = f'{outputs_directory}/dias_catalog_submit.json'
        if not os.path.isfile(gfsc_infos_file_name):
            raise CsiInternalError(
                'Missing infos file',
                'could not find expected JSON file that contains some info about GFSC')
        with open(gfsc_infos_file_name, 'r') as gfsc_infos_file:
            self.job.gfsc_infos = gfsc_infos_file.read()
       
        if self.job.generated_a_product():

            # Remove the info file before uploading the product in the bucket
            os.remove(gfsc_infos_file_name)

            self.logger.info(f'set the completion date for the GFSC product')
            json_set_by_worker = json.loads(self.job.gfsc_infos)
            self.job.completion_date = json_set_by_worker["resto"]["properties"]["completionDate"]

            self.logger.info(f'set sensing times for the GFSC product')
            self.job.sensing_start_date = datetime.strptime(
                json_set_by_worker[
                    "resto"][
                        "properties"][
                            "startDate"],
                '%Y-%m-%dT%H:%M:%S.%fZ'
            )
            self.job.sensing_end_date = datetime.strptime(
                json_set_by_worker[
                    "resto"][
                        "properties"][
                            "endDate"],
                '%Y-%m-%dT%H:%M:%S.%fZ'
                )

            # Upload QuickLook separatly on a different path than the products on the bucket
            # and remove it from the product folder
            self.logger.info(f'upload the GFSC QuickLook in the HR-S&I products bucket')
            (gfsc_bucket, gfsc_object_path) = GfscJobUtil.split_bucket_and_object_from_path(
                os.path.dirname(self.job.gfsc_path), prefix=True)
            quicklook_object_path = os.path.join(
                GfscJobUtil.get_quicklook_bucket_path(gfsc_object_path),
                os.path.basename(outputs_directory.rstrip(os.sep)),
                'thumbnail.png')

            quicklook_file_name = os.path.join(outputs_directory,'thumbnail.png')

            self.upload_file(
                csi_products_s3,
                gfsc_bucket,
                quicklook_file_name,
                quicklook_object_path
            )

            os.remove(quicklook_file_name)

            self.logger.info(f'upload the products in the HR-S&I products bucket')
            self.upload_directory(
                csi_products_s3,
                gfsc_bucket,
                outputs_directory,
                gfsc_object_path
            )

        self.logger.info(f'clean the local GFSC product directory')
        shutil.rmtree(outputs_directory)

        self.logger.info(f'update some job informations')
        self.job.patch(patch_foreign=True, logger_func=self.logger.debug)


# Function does the minimal stuff to get a valid job, an associate logger and
# launches the 'run' function which actually executes the tasks. This function
# catch errors from 'run' command and "sends" them to the database (with the
# status change).
def main(job_id: int):
    jobs: List[GfscJob] = GfscJob().id_eq(job_id).get()

    if len(jobs) == 0:
        # TODO how can we notify this very critical internal error? As we
        # found no job, this message will only appear on the stdout of the
        # Nomad job. Find a way to manage this.
        print(f'there is no job which ID is {job_id}')
        exit(1)

    job = jobs[0]
    logger = Logger(job.post_new_execution_info())

    # Create an instance of the RunWorker class to process the job..
    run_worker_instance = RunGfscWorker(job, logger)
    try:
        run_worker_instance.run()
        logger.info('run finished')
        error = None
    except Exception as exception:
        error = exception
        if not isinstance(error, (CsiInternalError, CsiExternalError)):
            logger.error(traceback.format_exc())
    finally:
        run_worker_instance.error_management_and_exit(
            error
        )
# End of mandatory methods