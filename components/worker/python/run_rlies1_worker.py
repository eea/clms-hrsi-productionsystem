import sys

assert sys.version_info >= (3, 0, 0)

import os
import pathlib
import subprocess
import string
import shutil
from datetime import datetime, timedelta
import time
import socket
import tarfile
import json
import traceback

import yaml

from typing import List

# Check if environment variable is set, exit in error if it's not
from ...common.python.util.sys_util import SysUtil
SysUtil.ensure_env_var_set('COSIMS_DB_HTTP_API_BASE_URL')

from ...common.python.database.model.job.rlies1_job import RlieS1Job

from ...common.python.database.model.job.job_status import JobStatus
from ...common.python.database.logger import Logger
from ...common.python.database.model.job.system_parameters import SystemPrameters
from ...common.python.util.exceptions import CsiInternalError, CsiExternalError
from ...common.python.util.dias_storage_util import DiasStorageUtil

from .run_worker_template import RunWorker


class RunRlieS1Worker(RunWorker):
    '''
    Class of "run_worker" object in charge of setting up a worker and run the 
    dockerized scientific Snow and Ice software on it to generate RLIE S1 products
    from S1 GRD products
    
    :param root_dir: path leading to the directory in which will be stored all 
        the temporary files needed and/or generated by the processing.
    '''

    def __init__(self, job, logger):

        # Call the parent constructor BEFORE all the attributes are initialized
        super().__init__(root_dir=os.environ['CSI_ROOT_DIR'])

        self.logger = logger                # Logger object

        # TODO : cast the correct job type
        self.job = job                      # Job instance of type RLIE S1
        self.local_job_dir = self.get_local_job_dir(self.job)
        self.docker_mount_dir = '/work'
        self.input_products = "inputs"         # Name of the input folder
        
        self.s1grd_local_path = os.path.join(self.local_job_dir, self.input_products, 'dynamic', os.path.basename(self.job.product_path))
        self.output_dir = os.path.join(self.local_job_dir, self.input_products, 'outputs', 'processing')
        self.local_input_directories = [self.local_job_dir] # List of directories in which are stored the input used during the processing
        # End of the specific members
        
        self.has_s2_intersection = None
        
    def to_docker_path(self, path_in):
        assert self.local_job_dir in path_in
        return path_in.replace(self.local_job_dir, self.docker_mount_dir)


# Start of specific methods
    def rename_quicklook_file(self, outputs_directory: str):
        # Find original quicklook name and path
        quicklook_tailing = "_QLK.png"
        product_id = os.path.basename(os.path.normpath(outputs_directory))
        quicklook_file_name = product_id + quicklook_tailing
        quicklook_file_path = f'{outputs_directory}/{quicklook_file_name}'

        # Rename quicklook
        new_outputs_directory = os.path.dirname(outputs_directory)
        new_outputs_directory = f'{new_outputs_directory}/quicklook/' + product_id
        pathlib.Path(new_outputs_directory).mkdir(parents=True, exist_ok=True)
        new_quicklook_file_path = f'{new_outputs_directory}/thumbnail.png'
        os.rename(quicklook_file_path, new_quicklook_file_path)

        return new_quicklook_file_path


    def up_one_directory(self, path: str):
        """Move file in path up one directory"""

        # Move the sub-elements at their parent directory level
        for sub_element in os.listdir(path):
            p = pathlib.Path(os.path.join(path, sub_element)).absolute()
            parent_dir = p.parents[1]

            # Skip the folder re-organisation, as the sub-folder to move is empty, 
            # and it already exists in the destination folder, and is not empty.
            if (
                not os.listdir(os.path.join(path, sub_element)) 
                and os.path.exists(parent_dir / p.name) 
                and os.listdir(parent_dir / p.name)
            ):
                self.logger.warning(f'Skipping sub-folder {os.path.join(path, sub_element)} '\
                    'move upward as it s empty  and the folder already exists in the destination !')
                break

            # Log a warning message if the subfolder to move is empty
            elif not os.listdir(os.path.join(path, sub_element)):
                self.logger.warning(f'The sub-folder {os.path.join(path, sub_element)} '\
                    'which is planned to be moved upward is empty !')

            # If the sub-folder to move is not empty and a folder already exists in the 
            # destination with the same name, we remove this one before moving the sub-folder.
            elif os.path.exists(parent_dir / p.name) and os.listdir(os.path.join(path, sub_element)):
                shutil.rmtree(parent_dir / p.name)

            p.rename(parent_dir / p.name)

        # Remove the original sub-elements parent directory
        shutil.rmtree(path)
# End of specific methods


# Start of mandatory methods
    def job_pre_processing(self):

        self.logger.info(f'Job pre-processing')

        # Update the python job object so that it is synchronized with the database,
        # because we will update some fields then apply a patch which needs an up to
        # date local version of job (in particular for some status related fields).

        # DONE : Replace this line with the correct job type class
        jobs: List[RlieS1Job] = RlieS1Job().id_eq(self.job.id).get()
        self.job = jobs[0]
        job_dir = self.get_local_job_dir(self.job)

        csi_s3, csi_buckets_names = DiasStorageUtil.get_csi_buckets_conf()
        eodata_s3, eodata_bucket = DiasStorageUtil.get_eoadata_bucket_conf()

        os.makedirs(os.path.join(self.local_job_dir, self.input_products, 'static'), exist_ok=True)
        os.makedirs(os.path.join(self.local_job_dir, self.input_products, 'dynamic'), exist_ok=True)
        os.makedirs(os.path.join(self.local_job_dir, 'outputs'), exist_ok=True)
        os.makedirs(os.path.join(self.local_job_dir, 'temp'), exist_ok=True)

        # DONE : Set correct path leading to the expected input product on expected storage
        product_bucket_path = self.job.product_path.lstrip('/eodata/')
        if product_bucket_path[-1] != '/':
            product_bucket_path += '/'

        self.logger.info(f'get product from EODATA bucket (product path: {product_bucket_path})')
        self.download_prefixed_objects(
            eodata_s3,
            eodata_bucket,
            product_bucket_path,
            os.path.dirname(self.s1grd_local_path),
            logger=self.logger)


        # ---------- Get the aux files ----------
        self.logger.info(f'get auxiliary files')
        aux_archive_loc = os.path.join(self.local_job_dir, self.input_products, 'static', 'rlie_s1_static_aux.tar.gz')
        self.download_file_from_bucket(
            csi_s3, csi_buckets_names['sip_aux'],
            'rlie_s1_static_aux.tar.gz',
            aux_archive_loc)
        with tarfile.open(aux_archive_loc) as tar:
            tar.extractall(os.path.join(self.local_job_dir, self.input_products, 'static'))
        os.unlink(aux_archive_loc)


    def job_processing(self):
        

        self.logger.info('get bucket paths')
        csi_s3, csi_buckets_names = DiasStorageUtil.get_csi_buckets_conf()
        
        #retrieve docker image
        self.logger.info('get docker image name')
        docker_image = SystemPrameters().get(self.logger.debug).docker_image_for_rliepart2_processing
        self.logger.info(' --> %s'%docker_image)
        json_geometries_file = os.path.join(self.local_job_dir, 'outputs', 'preprocessing', 'precomputed_product_geometries.json')

        #compute intersection
        self.logger.info('Compute S1/S2 precise intersection')

        pre_processing_dir = os.path.join(self.local_job_dir, 'outputs', 'preprocessing')
        if os.path.exists(pre_processing_dir):
            shutil.rmtree(pre_processing_dir)
        os.makedirs(pre_processing_dir)
        cmd = f'rlie_s1_preprocessing_compute_s1s2_intersection.py ' \
                f'--s1grd {self.to_docker_path(self.s1grd_local_path)} ' \
                f'--s2tiles_eea39_gdal_info {self.to_docker_path(os.path.join(self.local_job_dir, self.input_products, "static"))}/rlie_s1_static_aux/sentinel2tiles/s2tiles_eea39_gdal_info.json ' \
                f'--output_json {self.to_docker_path(json_geometries_file)} ' \
                f'--temp_dir {self.to_docker_path(os.path.join(self.local_job_dir, "temp"))}'
        self.run_processing_in_docker(
            image_name=docker_image,
            container_name=f'rlies1_preprocessing_{self.job.id}', #TBD: check how this is necessary
            command=cmd,
            volumes_binding={
                self.local_job_dir: { # The local working dir
                    'bind': self.docker_mount_dir,
                    'mode': 'rw'
                    }
                },
            logger=self.logger
            )
            
        self.logger.info(' -> reading intersection output')
        with open(json_geometries_file) as ds:
            dem_tiles_load = json.load(ds)
        if dem_tiles_load is None:
            dem_tiles_load = dict()
        dem_tiles_load = set(dem_tiles_load.keys())
        self.logger.info(f'Detailed intersection: {sorted(list(dem_tiles_load))}')
        self.has_s2_intersection = len(dem_tiles_load) > 0
        
        #exit processing if no intersection
        if not self.has_s2_intersection:
            return
            
        if dem_tiles_load != set(self.job.s2tile_ids_json):
            self.logger.info(f'Naive intersection: {sorted(list(self.job.s2tile_ids_json))}')
            self.logger.info('Naive and detailed intersection mismatch.')

        self.logger.info('Get necessary DEM files:')
        #get tile list necessary and load DEM files 
        dem_dir = os.path.join(self.local_job_dir, self.input_products, 'dynamic', 'dem')
        os.makedirs(dem_dir, exist_ok=True)
        for tile_id in dem_tiles_load:
            src_file = 'eu_dem/{0}/S2__TEST_AUX_REFDE2_T{0}_0001/S2__TEST_AUX_REFDE2_T{0}_0001.DBL.DIR/dem_20m.tif'.format(tile_id)
            self.logger.info(' -> %s'%src_file)
            self.download_file_from_bucket(
                csi_s3, csi_buckets_names['sip_aux'],
                src_file,
                os.path.join(dem_dir, 'dem_20m_%s.tif'%tile_id))
            
    
        #RLIE S1 processing
        self.logger.info('RLIE S1 processing')
        self.output_dir = os.path.join(self.local_job_dir, 'outputs', 'processing')
        os.makedirs(self.output_dir, exist_ok=True)

        cmd = f'rlie_s1_processing_chain.py ' \
                f'--s1grd {self.to_docker_path(self.s1grd_local_path)} ' \
                f'--output_dir {self.to_docker_path(self.output_dir)} ' \
                f'--euhydro_shapefile {self.to_docker_path(os.path.join(self.local_job_dir, self.input_products, "static"))}/rlie_s1_static_aux/EU_HYDRO/eu_hydro_3035.shp ' \
                f'--s2tiles_eea39_gdal_info {self.to_docker_path(os.path.join(self.local_job_dir, self.input_products, "static"))}/rlie_s1_static_aux/sentinel2tiles/s2tiles_eea39_gdal_info.json ' \
                f'--hrl_flags {self.to_docker_path(os.path.join(self.local_job_dir, self.input_products, "static"))}/rlie_s1_static_aux/HRL_FLAGS ' \
                f'--dem_path {self.to_docker_path(dem_dir)} ' \
                f'--precomputed_product_geometries_file {self.to_docker_path(json_geometries_file)} ' \
                f'--temp_dir {self.to_docker_path(os.path.join(self.local_job_dir, "temp"))} ' \
                f'--nprocs 4 ' \
                f'--verbose 2 ' \
                f'--overwrite'
        self.run_processing_in_docker(
            image_name=docker_image,
            container_name=f'rlies1_processing_{self.job.id}', #TBD: check how this is necessary
            command=cmd,
            volumes_binding={
                self.local_job_dir: { # The local working dir
                    'bind': self.docker_mount_dir,
                    'mode': 'rw'
                    }
                },
            logger=self.logger
            )
            


    def job_post_processing(self):
        
        self.logger.info(f'Job post-processing')
        
        csi_s3, csi_buckets_names = DiasStorageUtil.get_csi_buckets_conf()
        csi_products_s3, csi_products_bucket = DiasStorageUtil.get_product_bucket_conf()

        self.logger.info(f'set the completion date for the product')
        self.job.rlies1_products_completion_date = datetime.utcnow()
        
        yaml_product_file = os.path.join(self.output_dir, 'data', 'product_dict.yaml')
        if not self.has_s2_intersection: #case where there was no intersection with S2 tiles
            assert not os.path.exists(yaml_product_file), 'product dict should not exist (no S2 intersection)'
            product_dict = dict()
        else:
            with open(yaml_product_file) as ds:
                product_dict = yaml.load(ds)
            if product_dict is None: #case where there was an intersection with S2 tiles but ICE processing returned no products (maybe it never happens)
                self.logger.info(f'there was an intersection with S2 tiles but ICE processing returned no products')
                product_dict = dict()
            
            
        self.logger.info(f'upload the products in the HR-S&I products bucket')
        self.job.rlies1_product_paths_json = dict()
        self.job.rlies1_product_json_submitted_json = dict()
        for key, value in product_dict.items():
            self.logger.info('Product %s'%key)
            
            output_dir = 'CLMS/Pan-European/High_Resolution_Layers/Ice/RLIE_S1/%s'%self.job.measurement_date_start.strftime('%Y/%m/%d')
            input_path = os.path.join(self.output_dir, 'data', value)
            output_path = os.path.join(output_dir, value)
            
            self.logger.info('  -> Reading and deleting JSON')
            #load json info into job attr rlies1_product_json_submitted_json (to be saved to database at the end of job_post_processing)
            dias_catalog_submit_json = os.path.join(input_path, 'dias_catalog_submit.json')
            with open(dias_catalog_submit_json) as ds:
                self.job.rlies1_product_json_submitted_json[value] = json.load(ds)
            #delete json info file before product upload
            os.unlink(dias_catalog_submit_json)
            

            self.logger.info('  -> uploading and deleting thumbnail')
            #copy thumbnail file and delete it
            thumbnail_path_in = os.path.join(input_path, value + '_QLK.png')
            thumbnail_path_out = os.path.join('Preview', output_dir, value, 'thumbnail.png')
            self.upload_file(
                csi_products_s3,
                csi_products_bucket,
                thumbnail_path_in,
                thumbnail_path_out)
            #delete thumbnail info file before product upload
            os.unlink(thumbnail_path_in)
            
            self.logger.info('  -> uploading product on bucket')
            self.upload_directory(
                csi_products_s3,
                csi_products_bucket,
                input_path,
                output_dir)
            self.job.rlies1_product_paths_json[value] = output_path
            self.job.rlies1_product_json_submitted_json[value]['resto']['properties']['host_base'] = 's3.waw2-1.cloudferro.com'
            self.job.rlies1_product_json_submitted_json[value]['resto']['properties']['productIdentifier'] = os.path.join('/eodata/HRSI', self.job.rlies1_product_paths_json[value])
            self.job.rlies1_product_json_submitted_json[value]['resto']['properties']['thumbnail'] = thumbnail_path_out
            self.job.rlies1_product_json_submitted_json[value]['resto']['properties']['s3_bucket'] = csi_products_bucket
        
        self.logger.info(f'update some job informations')
        self.job.patch(patch_foreign=True, logger_func=self.logger.debug)




# Function does the minimal stuff to get a valid job, an associate logger and
# launches the 'run' function which actually executes the tasks. This function
# catch errors from 'run' command and "sends" them to the database (with the
# status change).
def main(job_id: int):
    # TODO : Replace this line with the correct job type class
    jobs: List[RlieS1Job] = RlieS1Job().id_eq(job_id).get()

    if len(jobs) == 0:
        print(f'there is no job which ID is {job_id}')
        exit(1)

    job = jobs[0]
    logger = Logger(job.post_new_execution_info())

    # Create an instance of the RunWorker class to process the job..
    # TODO : Replace this line with the correct run worker class
    run_worker_instance = RunRlieS1Worker(job, logger)

    try:
        run_worker_instance.run()
        logger.info('run finished')
        error = None
    except Exception as exception:
        error = exception
        if not isinstance(error, (CsiInternalError, CsiExternalError)):
            logger.error(traceback.format_exc())
    finally:
        run_worker_instance.error_management_and_exit(
            error
        )
# End of mandatory methods
