import sys

assert sys.version_info >= (3, 0, 0)

import os
import pathlib
import subprocess
import string
import shutil
import datetime
import time
import socket
import tarfile
import traceback

import yaml

from typing import List

# Check if environment variable is set, exit in error if it's not
from ...common.python.util.sys_util import SysUtil
SysUtil.ensure_env_var_set('COSIMS_DB_HTTP_API_BASE_URL')

from ...common.python.database.model.job.fsc_rlie_job import FscRlieJob
from ...common.python.database.model.job.job_status import JobStatus
from ...common.python.database.model.job.l2a_status import L2aStatus
from ...common.python.database.model.job.maja_mode import MajaMode
from ...common.python.database.logger import Logger
from ...common.python.database.model.job.system_parameters import SystemPrameters
from ...common.python.util.fsc_rlie_job_util import FscRlieJobUtil
from ...common.python.util.exceptions import CsiInternalError, CsiExternalError
from ...common.python.util.dias_storage_util import DiasStorageUtil

from .run_worker_template import RunWorker


class RunL1cWorker(RunWorker):
    '''
    Class of "run_worker" object in charge of setting up a worker and run the 
    dockerized scientific Snow and Ice software on it to generate FSC/RLIE 
    products from L1C, L2A input.

    :param root_dir: path leading to the directory in which will be stored all 
        the temporary files needed and/or generated by the processing.
    '''

    def __init__(self, job, logger):

        # Call the parent constructor BEFORE all the attributes are initialized
        super().__init__(root_dir=os.environ['CSI_ROOT_DIR'])

        self.input_products = "l1c"         # Name of the input folder
        self.job = job                      # Job instance of type FscRlieJob 
        self.logger = logger                # Logger object
        self.parameters_file = None         # Path leading to the processing parameter file 
        self.parameters = None              # Dictionary containing the processing parameters
        self.local_input_directories = None # List of directories in which are stored the input used during the processing
        self.results_are_available = None   # Boolean notifying if products have been generated by the processing


    def rename_quicklook_file(self, outputs_directory: str):
        # Find original quicklook name and path
        quicklook_tailing = "_QLK.png"
        product_id = os.path.basename(os.path.normpath(outputs_directory))
        quicklook_file_name = product_id + quicklook_tailing
        quicklook_file_path = f'{outputs_directory}/{quicklook_file_name}'

        # Rename quicklook
        new_outputs_directory = os.path.dirname(outputs_directory)
        new_outputs_directory = f'{new_outputs_directory}/quicklook/' + product_id
        pathlib.Path(new_outputs_directory).mkdir(parents=True, exist_ok=True)
        new_quicklook_file_path = f'{new_outputs_directory}/thumbnail.png'
        os.rename(quicklook_file_path, new_quicklook_file_path)

        return new_quicklook_file_path


    def assert_l1c_lists_coherence(self, product_ids: list, product_eodata_paths: list):
        # Ensure both lists have the same dimension
        if len(product_ids) != len(product_eodata_paths):
            raise CsiInternalError(
                'L1C inputs inconsistency',
                f'Different number of products were set between "l1c_id_list" and "l1c_path_list".'
                )
        
        # Ensure L1C IDs and paths are arranged in the same order
        for i in range(len(product_ids)):
            if product_ids[i] not in product_eodata_paths[i]:
                raise CsiInternalError(
                    'L1C inputs inconsistency',
                    f'L1C products in the "l1c_id_list" and "l1c_path_list" are not'\
                    ' arranged in the same order.'
                    )


    def up_one_directory(self, path: str):
        """Move file in path up one directory"""

        # Move the sub-elements at their parent directory level
        for sub_element in os.listdir(path):
            p = pathlib.Path(os.path.join(path, sub_element)).absolute()
            parent_dir = p.parents[1]

            # Skip the folder re-organisation, as the sub-folder to move is empty, 
            # and it already exists in the destination folder, and is not empty.
            if (
                not os.listdir(os.path.join(path, sub_element)) 
                and os.path.exists(parent_dir / p.name) 
                and os.listdir(parent_dir / p.name)
            ):
                self.logger.warning(f'Skipping sub-folder {os.path.join(path, sub_element)} '\
                    'move upward as it s empty  and the folder already exists in the destination !')
                break

            # Log a warning message if the subfolder to move is empty
            elif not os.listdir(os.path.join(path, sub_element)):
                self.logger.warning(f'The sub-folder {os.path.join(path, sub_element)} '\
                    'which is planned to be moved upward is empty !')

            # If the sub-folder to move is not empty and a folder already exists in the 
            # destination with the same name, we remove this one before moving the sub-folder.
            elif os.path.exists(parent_dir / p.name) and os.listdir(os.path.join(path, sub_element)):
                shutil.rmtree(parent_dir / p.name)

            p.rename(parent_dir / p.name)

        # Remove the original sub-elements parent directory
        shutil.rmtree(path)


    def job_pre_processing(self):
        # TODO check if the downloaded files and directories exist as expected.

        # Update the python job object so that it is synchronized with the database,
        # because we will update some fields then apply a patch which needs an up to
        # date local version of job (in particular for some status related fields).
        jobs: List[FscRlieJob] = FscRlieJob().id_eq(self.job.id).get()
        self.job = jobs[0]

        self.parameters = {}

        csi_s3, csi_buckets_names = DiasStorageUtil.get_csi_buckets_conf()
        eodata_s3, eodata_bucket = DiasStorageUtil.get_eoadata_bucket_conf()

        job_dir = self.get_local_job_dir(self.job)
        tile_id = self.job.tile_id
        self.parameters['tile_id'] = tile_id

        # ---------- Get the L1C product ----------
        maja_mode_code = self.job.maja_mode
        if maja_mode_code == None:
            raise CsiInternalError(
                'Missing maja mode',
                f'the maja mode for the job must be set (got "None")'
                )
        maja_mode = MajaMode.to_string(maja_mode_code)

        # Retrieve inputs L1C IDs and paths 
        if maja_mode == MajaMode.backward.name:
            if self.job.l1c_path_list is None or self.job.l1c_id_list is None:
                raise CsiInternalError(
                    'Missing L1C input',
                    f'as Maja mode is "backward": there must be a list of L1C files as input'
                    )
            else:
                product_eodata_paths = self.job.l1c_path_list.split(';')
                product_ids = self.job.l1c_id_list.split(';')
                product_local_paths = [f"/work/jobs/{self.job.unique_id}/l1c/{product_id}.SAFE" for product_id in product_ids]
                self.parameters['product_file_path'] = product_local_paths
        else:
            product_eodata_paths = [self.job.l1c_path]
            product_ids = [self.job.l1c_id]
            self.parameters['product_file_path'] = f"/work/jobs/{self.job.unique_id}/l1c/{self.job.l1c_id}.SAFE"

        # Ensure that list of L1C IDs and paths are coherent
        self.assert_l1c_lists_coherence(product_ids, product_eodata_paths)

        l1c_product_local_directory = self.get_input_product_local_directory(self.job, self.input_products)

        # List of directories  where we donwload some inputs and that we don't need
        # to store in the results bucket at the end of the processing.
        self.local_input_directories = []

        # Get the input product and put it in the expected input directory for S&I
        # processing softwares.
        for i in range(len(product_eodata_paths)):
            # Convert
            #   "/eodata/path/to/l1c/in/eodata/bucket"
            # to
            #   "path/to/l1c/in/eodata/bucket"
            product_eodata_path = self.remove_prefix(product_eodata_paths[i], '/eodata/')
            product_eodata_path = self.append_trailing_slash_if_missing(product_eodata_path)

            l1c_product_path = os.path.join(l1c_product_local_directory, f'{product_ids[i]}.SAFE')
            if os.path.exists(l1c_product_path):
                # Delete the product as it might be corrupted
                shutil.rmtree(l1c_product_path)
            self.logger.info(f'create product directory ({l1c_product_local_directory})')
            pathlib.Path(l1c_product_local_directory).mkdir(parents=True, exist_ok=True)
            self.logger.info(f'get product from EODATA bucket (product path: {product_eodata_path})')
            self.download_prefixed_objects(
                eodata_s3,
                eodata_bucket,
                product_eodata_path,
                l1c_product_local_directory,
                logger=self.logger)

        self.local_input_directories.append(l1c_product_local_directory)

        # ---------- Get the aux files ----------
        self.logger.info(f'get auxiliary files')
        aux_local_file_path = self.compute_local_file_path(
            f'csi_aux/csi_aux_{tile_id}.tar', 
            f'{job_dir}',
            logger=self.logger)
        self.download_file_from_bucket(
            csi_s3, csi_buckets_names['sip_aux'],
            f'csi_aux/csi_aux_{tile_id}.tar',
            aux_local_file_path)
        with tarfile.open(f'{job_dir}/csi_aux_{tile_id}.tar') as tar:
            tar.extractall(f'{job_dir}')

        # Some patch actions to have correct folder tree architecture
        self.up_one_directory(os.path.join(job_dir, tile_id))


        # Add the DEM to the list of folders to clean
        dem_local_directory = f'{job_dir}/eu_dem'
        self.local_input_directories.append(dem_local_directory)


        # Add the TCD to the list of folders to clean
        tcd_local_directory = f'{job_dir}/tree_cover_density'
        self.local_input_directories.append(tcd_local_directory)


        # Add the EU Hydro to the list of folders to clean
        eu_hydro_local_directory = f'{job_dir}/eu_hydro'
        self.local_input_directories.append(eu_hydro_local_directory)


        # Check if  EU hydro shapefile exists
        generate_ice_product = 'true'
        self.logger.info(f'check if there is a shapefile for this tile')
        no_shapefile_path = f'{job_dir}/eu_hydro/shapefile/nodata'
        river_shapefile = ''
        if not os.path.exists(no_shapefile_path):
            self.logger.info(f'  -> shapefile exists for this tile')
            river_shapefile = f'{self.get_container_work_dir()}/jobs/{self.job.unique_id}/eu_hydro/shapefile/eu_hydro_{tile_id}.shp'
        else:
            generate_ice_product = 'false'
            self.logger.info(f'  -> there is no shapefile for this tile')

        self.parameters['river_shapefile'] = river_shapefile


        # Check if the HRL QC flags file exists
        self.logger.info(f'check if there is a HRL QC flags file this tile')
        no_hrl_qc_flags_path = f'{job_dir}/hrl_qc_flags/nodata'
        hrl_flags_file = ''
        if not os.path.exists(no_hrl_qc_flags_path):
            self.logger.info(f'  -> HRL QC flags file exists for this tile')
            hrl_qc_local_directory = f'{job_dir}/hrl_qc_flags'
            self.local_input_directories.append(hrl_qc_local_directory)
            hrl_flags_file = f'{self.get_container_work_dir()}/jobs/{self.job.unique_id}/hrl_qc_flags/hrl_qc_flags_{tile_id}.tif'
        else:
            generate_ice_product = 'false'
            self.logger.info(f'  -> there is no HRL QC flags file for this tile')

        self.parameters['hrl_flags_file'] = hrl_flags_file
        self.parameters['generate_ice_product'] = generate_ice_product


        # ---------- Get the L2A metadata files ----------
        l2a_basename = "no_l2a_input_data"
        l2a_path_in = self.job.l2a_path_in
        if l2a_path_in == None:
            self.logger.info(f'there is no input L2A metadata for this job')
            if maja_mode == MajaMode.nominal.name:
                raise CsiInternalError(
                    'Missing L2A input',
                    f'as Maja mode is "nominal": there must be some L2A metadata'
                    )
            l2a_file = ''
        else:
            self.logger.info(f'download input L2A metadata for this job from {l2a_path_in}')

            # l2a_path_in contains something like that:
            #    hidden_value/26SNH/L2A/reference/S2B_MSIL1C_20200408T125029_N0209_R095_T26SNH_20200408T161410/init
            # This "directory" in the bucket contains the actual directory with
            # the L1A metadata, which name is something like:
            #    SENTINEL2B_20200408-125105-587_L2A_T26SNH_C_V1-0
            # So, we want to download from the hidden_value bucket the "directory
            # object" which key is:
            #    26SNH/L2A/reference/S2B_MSIL1C_20200408T125029_N0209_R095_T26SNH_20200408T161410/init/SENTINEL2B_20200408-125105-587_L2A_T26SNH_C_V1-0
            # And put it in a local L2A directory which is something like:
            #    /opt/csi/work/jobs/1505/l2a/SENTINEL2B_20200408-125105-587_L2A_T26SNH_C_V1-0
            # And that will be ween by the docker container as:
            #    /work/jobs/1505/l2a/SENTINEL2B_20200408-125105-587_L2A_T26SNH_C_V1-0
            # The following code does just that.

            l2a_target_local_dir = os.path.join(job_dir, 'l2a')

            (l2a_bucket, object_path) = FscRlieJobUtil.split_bucket_and_object_from_path(l2a_path_in)
            self.download_prefixed_objects(
                csi_s3, l2a_bucket,
                self.append_trailing_slash_if_missing(object_path),
                l2a_target_local_dir,
                logger=self.logger)
            self.local_input_directories.append(l2a_target_local_dir)

            # With the example above, parent_basename = 'init', and is the directory
            # that contains the actual L2A metadata directory.
            parent_basename = os.path.basename(object_path)

            # This is the complete local path of the "init" directory.
            downloaded_directory = os.path.join(l2a_target_local_dir, parent_basename)

            # Get its content which must be a unique directory
            content_list = os.listdir(downloaded_directory)
            if len(content_list) != 1:
                raise CsiInternalError(
                    'Bad L2A input content',
                    (
                        f'there must be only one directory in the object path given to '
                        f'get the L2A metadata (got {len(content_list)})'
                        )
                    )

            # Now, at last, we can get the L2A metadata actual name
            l2a_basename = content_list[0]

            # We don't want to keep the name "init" in the target path, move its
            # content one directory up and remove it.
            try:
                shutil.move(os.path.join(downloaded_directory, l2a_basename), l2a_target_local_dir)
            except shutil.Error:
                shutil.rmtree(os.path.join(l2a_target_local_dir, l2a_basename))
                shutil.move(os.path.join(downloaded_directory, l2a_basename), l2a_target_local_dir)

            os.rmdir(downloaded_directory)

            # Done.

            l2a_file = f'{self.get_container_work_dir()}/jobs/{self.job.unique_id}/l2a/{l2a_basename}'
            self.logger.info(f'L2A metadata directory name is {l2a_basename}')

        self.parameters['maja_mode'] = maja_mode
        self.parameters['l2a_file'] = l2a_file

        # ---------- Get the Maja static files ----------
        self.logger.info(f'get the Maja input configuration files')
        work_dir = self.get_work_dir()
        maja_static_local_file_path = self.compute_local_file_path(
            f'si_software/maja_config_input_files/maja_4.5.3/maja_static.tar', 
            f'{work_dir}/maja',
            logger=self.logger)
        self.download_file_from_bucket(
            csi_s3, csi_buckets_names['infra'],
            f'si_software/maja_config_input_files/maja_4.5.3/maja_static.tar',
            maja_static_local_file_path)
        with tarfile.open(f'{work_dir}/maja/maja_static.tar') as tar:
            tar.extractall(f'{work_dir}/maja')
        maja_userconf_local_file_path = self.compute_local_file_path(
            f'si_software/maja_config_input_files/maja_4.5.3/maja_userconf.tar', 
            f'{work_dir}/maja',
            logger=self.logger)
        self.download_file_from_bucket(
            csi_s3, csi_buckets_names['infra'],
            f'si_software/maja_config_input_files/maja_4.5.3/maja_userconf.tar',
            maja_userconf_local_file_path)
        with tarfile.open(f'{work_dir}/maja/maja_userconf.tar') as tar:
            tar.extractall(f'{work_dir}/maja')
        self.local_input_directories.append(f'{work_dir}/maja')

        # ---------- Parameters update & sip parameters file creation ----------
        self.parameters['product_mode_overide'] = FscRlieJobUtil.compute_product_quality_flag(self.job)
        self.parameters['job_unique_id'] = self.job.unique_id

        self.logger.info(f'create the S&I processing parameters file')
        self.create_sip_parameters_file()


    def create_sip_parameters_file(self):
        # Insert some values from the parameters dictionary in a template for the
        # S&I processing parameter file.
        here = os.path.dirname(os.path.realpath(__file__))
        parameters_template_file = os.path.join(here, 'fsc_rlie_processing_parameters_template.yaml')
        parameters_template = pathlib.Path(parameters_template_file).read_text()
        parameters_string = string.Template(parameters_template).substitute(self.parameters)
        self.parameters_file = os.path.join(
            'jobs', str(self.job.unique_id), 'processings_parameters.yaml')
        pathlib.Path(os.path.join(self.get_work_dir(), self.parameters_file)).write_text(parameters_string)


    def job_processing(self):
        docker_image = SystemPrameters().get(self.logger.debug).docker_image_for_si_processing
        container_parameter_file_path = os.path.join(self.get_container_work_dir(), self.parameters_file)
        processing_exe_name = 'fsc_rlie_processing_chain.py'
        container_command = f'{processing_exe_name} "{container_parameter_file_path}"'
        work_dir = self.get_work_dir()

        si_processing_status = self.run_processing_in_docker(
            image_name=docker_image,
            container_name=f'si_processing_{self.job.id}',
            command=container_command,
            volumes_binding={
                f'{work_dir}': { # The local working dir
                    'bind': self.get_container_work_dir(), # the dir in the container, something like '/work'
                    'mode': 'rw'
                    }
                },
            logger=self.logger
            )

        self.job.si_processing_image = docker_image
        self.job.maja_return_code = si_processing_status
        self.job.patch(patch_foreign=True, logger_func=self.logger.debug)

        # Check the status returned by the processing
        self.check_si_processing_status(si_processing_status)


    def check_si_processing_status(self, si_processing_status):
        self.results_are_available = False
        if si_processing_status == 0:
            self.logger.info(f'processing finished with no error, proceed')
            self.results_are_available = True
        elif si_processing_status == 200:
            self.logger.info(f'Maja processing raised an alert about a high cloud cover')
            self.logger.info(f'FSC and RLIE products can\'t be computed and there is no L2A metadata to keep')
            self.results_are_available = False
        elif si_processing_status == 201:
            self.logger.info(f'Maja processing raised an alert about an incomplete or a corrupted L1C input file')
            self.logger.info(f'FSC and RLIE products can\'t be computed and there is no L2A metadata to keep')
            self.results_are_available = False
        elif (si_processing_status == 202 or si_processing_status == 204):
            self.logger.info(f'Maja processing raised an alert about an incorrect L1C input file')
            self.logger.info(f'FSC and RLIE products can\'t be computed and there is no L2A metadata to keep')
            self.results_are_available = False
        elif si_processing_status == 203:
            self.logger.info(f'Processing raised an alert on L1C detection')
            self.logger.info(f'L1C corrupted data prevent MAJA from running')
            self.results_are_available = False
        elif si_processing_status == 205:
            self.logger.info(f'Processing raised an alert about a missing band')
            self.logger.info(f'L1C corrupted data prevent MAJA from running')
            self.results_are_available = False
        elif si_processing_status == 206:
            self.logger.info(f'Processing raised an alert on L1C parsing')
            self.logger.info(f'L1C corrupted data prevent MAJA from running')
        elif si_processing_status == 207:
            self.logger.info(f'Processing raised an alert about too many "NoData" pixels in the L2A product')
            self.logger.info(f'FSC and RLIE products can\'t be computed and there is no L2A metadata to keep')
            self.results_are_available = False

        elif si_processing_status < 100:
            message = (
                f'the S&I processing finished with an unrecoverable error (status '
                f'code = {si_processing_status})'
            )
            raise CsiInternalError('SIP unrecoverable error', message)
        elif si_processing_status < 200:
            message = (
                f'the S&I processing finished with an expected error (status code = '
                f'{si_processing_status}), we can try to run processing again'
            )
            raise CsiExternalError('SIP expected error', message)


    def manage_results(self, completion_date: str, ice_results_are_available: bool):

        csi_s3, csi_buckets_names = DiasStorageUtil.get_csi_buckets_conf()
        csi_products_s3, csi_products_bucket = DiasStorageUtil.get_product_bucket_conf()

        local_results_dir = self.get_local_results_dir(self.job)
        with open(f'{local_results_dir}/data/product_dict.yaml', 'r') as file:
            outputs_products = yaml.safe_load(file)

        l2a_product = outputs_products['l2a']

        self.logger.info(f'put the L2A metadata in the CoSIMS object storage')
        l2a_outputs_directory = os.path.join(
            local_results_dir, 'data', l2a_product)

        l2a_path_out = self.job.l2a_path_out
        (l2a_bucket, l2a_object_path) = FscRlieJobUtil.split_bucket_and_object_from_path(l2a_path_out)
        self.upload_directory(
            csi_s3,
            l2a_bucket,
            l2a_outputs_directory,
            l2a_object_path
        )
        self.logger.info(f'clean the local L2A directory')
        shutil.rmtree(l2a_outputs_directory)

        fsc_product_name = outputs_products['fsc']
        fsc_outputs_directory = os.path.join(
            local_results_dir, 'data', fsc_product_name)

        self.logger.info(f'set the completion date for the FSC product')
        self.job.fsc_completion_date = completion_date

        self.logger.info(f'set the bucket path for the FSC product')
        FscRlieJobUtil.set_product_bucket_paths(
            self.job, 
            "FSC",
            fsc_product_name,
            logger_func=self.logger.debug
        )

        self.logger.info(f'update the number of produced L2A since last init')
        self.job.n_l2a_produced_since_last_init += 1

        self.logger.info(f'get some infos about the FSC results from JSON file')
        fsc_infos_file_name = f'{fsc_outputs_directory}/dias_catalog_submit.json'
        if not os.path.isfile(fsc_infos_file_name):
            raise CsiInternalError(
                'Missing infos file',
                'could not find expected JSON file that contains some info about FSC')
        with open(fsc_infos_file_name, 'r') as fsc_infos_file:
            self.job.fsc_infos = fsc_infos_file.read()

        # Remove the info file before uploading the product in the bucket
        os.remove(fsc_infos_file_name)

        # Rename QuickLook file to a generic name and put it in a separate folder
        fsc_quicklook_file_name = self.rename_quicklook_file(fsc_outputs_directory)

        # Upload QuickLook separatly on a different path than the products on the bucket
        self.logger.info(f'upload the FSC QuickLook in the HR-S&I products bucket')
        (fsc_bucket, fsc_object_path) = FscRlieJobUtil.split_bucket_and_object_from_path(
            os.path.dirname(self.job.fsc_path), prefix=True)

        # TODO SHOULD : store quicklook path on the bucket in job parameters, 
        # set during configuration as results paths
        fsc_quicklook_path = FscRlieJobUtil.get_quicklook_bucket_path(fsc_object_path)

        self.upload_directory(
            csi_products_s3,
            fsc_bucket,
            os.path.dirname(fsc_quicklook_file_name),
            fsc_quicklook_path
        )

        self.logger.info(f'upload the FSC products in the HR-S&I products bucket')
        self.upload_directory(
            csi_products_s3,
            fsc_bucket,
            fsc_outputs_directory,
            fsc_object_path
        )
        self.logger.info(f'clean the local FSC product directory')
        shutil.rmtree(fsc_outputs_directory)

        if ice_results_are_available:
            rlie_product_name = outputs_products['rlie']
            rlie_outputs_directory = os.path.join(
                local_results_dir, 'data', rlie_product_name)

            self.logger.info(f'set the completion date for the RLIE product')
            self.job.rlie_completion_date = completion_date

            self.logger.info(f'set the bucket path for the RLIE product')
            FscRlieJobUtil.set_product_bucket_paths(
                self.job, 
                "RLIE",
                rlie_product_name,
                logger_func=self.logger.debug
            )

            self.logger.info(f'get some infos about the RLIE results from JSON file')
            rlie_infos_file_name = f'{rlie_outputs_directory}/dias_catalog_submit.json'
            if not os.path.isfile(rlie_infos_file_name):
                raise CsiInternalError(
                    'Missing infos file',
                    'could not find expected JSON file that contains some info about RLIE')
            with open(rlie_infos_file_name, 'r') as rlie_infos_file:
                self.job.rlie_infos = rlie_infos_file.read()

            # Remove the info file before uploading the product in the bucket
            os.remove(rlie_infos_file_name)

            # Rename QuickLook file to a generic name and put it in a separate folder
            rlie_quicklook_file_name = self.rename_quicklook_file(rlie_outputs_directory)

            # Upload QuickLook separatly on a different path than the products on the bucket
            self.logger.info(f'upload the RLIE QuickLook in the HR-S&I products bucket')
            (rlie_bucket, rlie_object_path) = FscRlieJobUtil.split_bucket_and_object_from_path(
                os.path.dirname(self.job.rlie_path), prefix=True)

            # TODO SHOULD : store quicklook path on the bucket in job parameters, 
            # set during configuration as results paths
            rlie_quicklook_path = FscRlieJobUtil.get_quicklook_bucket_path(rlie_object_path)

            self.upload_directory(
                csi_products_s3,
                rlie_bucket,
                os.path.dirname(rlie_quicklook_file_name),
                rlie_quicklook_path
            )

            self.logger.info(f'upload the RLIE products in the HR-S&I products bucket')
            self.upload_directory(
                csi_products_s3,
                rlie_bucket,
                rlie_outputs_directory,
                rlie_object_path
            )
            self.logger.info(f'clean the local RLIE product directory')
            shutil.rmtree(rlie_outputs_directory)


    def job_post_processing(self):

        completion_date = datetime.datetime.now()

        if not self.results_are_available:
            self.job.l2a_status = L2aStatus.generation_aborted
        else:
            self.job.l2a_status = L2aStatus.generated
            ice_results_are_available = (
                self.parameters['generate_ice_product'] == 'true'
                )
            self.manage_results(completion_date, ice_results_are_available)
        
        self.logger.info(f'update some job informations')
        self.job.patch(patch_foreign=True, logger_func=self.logger.debug)


# Function does the minimal stuff to get a valid job, an associate logger and
# launches the 'run' function which actually executes the tasks. This function
# catch errors from 'run' command and "sends" them to the database (with the
# status change).
def main(job_id: int):
    jobs: List[FscRlieJob] = FscRlieJob().id_eq(job_id).get()

    if len(jobs) == 0:
        # TODO how can we notify this very critical internal error? As we
        # found no job, this message will only appear on the stdout of the
        # Nomad job. Find a way to manage this.
        print(f'there is no job which ID is {job_id}')
        exit(1)

    job = jobs[0]
    logger = Logger(job.post_new_execution_info())

    # Create an instance of the RunWorker class to process the job.
    run_worker_instance = RunL1cWorker(job, logger)

    try:
        run_worker_instance.run()
        logger.info('run finished')
        error = None
    except Exception as exception:
        error = exception
        if not isinstance(error, (CsiInternalError, CsiExternalError)):
            logger.error(traceback.format_exc())
    finally:
        run_worker_instance.error_management_and_exit(
            error
        )
