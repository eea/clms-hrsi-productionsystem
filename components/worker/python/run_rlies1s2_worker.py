import sys

assert sys.version_info >= (3, 0, 0)

import os
import pathlib
import subprocess
import string
import shutil
from datetime import datetime, timedelta
import time
import socket
import tarfile
import json
import traceback

import yaml

from typing import List

# Check if environment variable is set, exit in error if it's not
from ...common.python.util.sys_util import SysUtil
SysUtil.ensure_env_var_set('COSIMS_DB_HTTP_API_BASE_URL')

from ...common.python.database.model.job.rlies1s2_job import RlieS1S2Job

from ...common.python.database.model.job.job_status import JobStatus
from ...common.python.database.logger import Logger
from ...common.python.database.model.job.system_parameters import SystemPrameters
from ...common.python.util.exceptions import CsiInternalError, CsiExternalError
from ...common.python.util.dias_storage_util import DiasStorageUtil
from ...common.python.util.fsc_rlie_job_util import FscRlieJobUtil

from .run_worker_template import RunWorker


class RunRlieS1S2Worker(RunWorker):
    '''
    Class of "run_worker" object in charge of setting up a worker and run the 
    dockerized scientific Snow and Ice software on it to generate RLIE S1 products
    from S1 GRD products
    
    :param root_dir: path leading to the directory in which will be stored all 
        the temporary files needed and/or generated by the processing.
    '''

    def __init__(self, job, logger):

        # Call the parent constructor BEFORE all the attributes are initialized
        super().__init__(root_dir=os.environ['CSI_ROOT_DIR'])

        self.logger = logger                # Logger object

        # TODO : cast the correct job type
        self.job = job                      # Job instance of type RLIE S1
        self.local_job_dir = self.get_local_job_dir(self.job)
        self.docker_mount_dir = '/work'
        self.input_products = "inputs"         # Name of the input folder
        
        self.rlies1_local_path = os.path.join(self.local_job_dir, self.input_products, 'dynamic', 'RLIE_S1')
        self.rlies2_local_path = os.path.join(self.local_job_dir, self.input_products, 'dynamic', 'RLIE_S2')
        self.output_dir = os.path.join(self.local_job_dir, self.input_products, 'outputs', 'processing')
        self.local_input_directories = [self.local_job_dir] # List of directories in which are stored the input used during the processing
        # End of the specific members
        
        
    def to_docker_path(self, path_in):
        assert self.local_job_dir in path_in
        return path_in.replace(self.local_job_dir, self.docker_mount_dir)


# Start of specific methods
    def rename_quicklook_file(self, outputs_directory: str):
        # Find original quicklook name and path
        quicklook_tailing = "_QLK.png"
        product_id = os.path.basename(os.path.normpath(outputs_directory))
        quicklook_file_name = product_id + quicklook_tailing
        quicklook_file_path = f'{outputs_directory}/{quicklook_file_name}'

        # Rename quicklook
        new_outputs_directory = os.path.dirname(outputs_directory)
        new_outputs_directory = f'{new_outputs_directory}/quicklook/' + product_id
        pathlib.Path(new_outputs_directory).mkdir(parents=True, exist_ok=True)
        new_quicklook_file_path = f'{new_outputs_directory}/thumbnail.png'
        os.rename(quicklook_file_path, new_quicklook_file_path)

        return new_quicklook_file_path


    def up_one_directory(self, path: str):
        """Move file in path up one directory"""

        # Move the sub-elements at their parent directory level
        for sub_element in os.listdir(path):
            p = pathlib.Path(os.path.join(path, sub_element)).absolute()
            parent_dir = p.parents[1]

            # Skip the folder re-organisation, as the sub-folder to move is empty, 
            # and it already exists in the destination folder, and is not empty.
            if (
                not os.listdir(os.path.join(path, sub_element)) 
                and os.path.exists(parent_dir / p.name) 
                and os.listdir(parent_dir / p.name)
            ):
                self.logger.warning(f'Skipping sub-folder {os.path.join(path, sub_element)} '\
                    'move upward as it s empty  and the folder already exists in the destination !')
                break

            # Log a warning message if the subfolder to move is empty
            elif not os.listdir(os.path.join(path, sub_element)):
                self.logger.warning(f'The sub-folder {os.path.join(path, sub_element)} '\
                    'which is planned to be moved upward is empty !')

            # If the sub-folder to move is not empty and a folder already exists in the 
            # destination with the same name, we remove this one before moving the sub-folder.
            elif os.path.exists(parent_dir / p.name) and os.listdir(os.path.join(path, sub_element)):
                shutil.rmtree(parent_dir / p.name)

            p.rename(parent_dir / p.name)

        # Remove the original sub-elements parent directory
        shutil.rmtree(path)
# End of specific methods


# Start of mandatory methods
    def job_pre_processing(self):

        # Update the python job object so that it is synchronized with the database,
        # because we will update some fields then apply a patch which needs an up to
        # date local version of job (in particular for some status related fields).

        # DONE : Replace this line with the correct job type class
        jobs: List[RlieS1S2Job] = RlieS1S2Job().id_eq(self.job.id).get()
        self.job = jobs[0]
        job_dir = self.get_local_job_dir(self.job)

        csi_s3, csi_buckets_names = DiasStorageUtil.get_csi_buckets_conf()
        csi_products_s3, csi_products_bucket = DiasStorageUtil.get_product_bucket_conf()

        os.makedirs(os.path.join(self.local_job_dir, self.input_products, 'static'), exist_ok=True)
        os.makedirs(os.path.join(self.local_job_dir, self.input_products, 'dynamic'), exist_ok=True)
        os.makedirs(os.path.join(self.local_job_dir, 'outputs'), exist_ok=True)
        os.makedirs(os.path.join(self.local_job_dir, 'temp'), exist_ok=True)
        
        # ---------- Get the aux files ----------
        self.logger.info(f'get auxiliary files')
        aux_archive_loc = os.path.join(self.local_job_dir, self.input_products, 'static', 'rlie_s1_static_aux.tar.gz')
        self.download_file_from_bucket(
            csi_s3, csi_buckets_names['sip_aux'],
            'rlie_s1_static_aux.tar.gz',
            aux_archive_loc)
        with tarfile.open(aux_archive_loc) as tar:
            tar.extractall(os.path.join(self.local_job_dir, self.input_products, 'static'))
        os.unlink(aux_archive_loc)

        #get rlies1 and rlies2 products
        self.logger.info('Getting rlies1 products:')
        for path_loc in self.job.rlies1_product_paths_json:
            self.logger.info(' -> %s'%path_loc)
            self.download_prefixed_objects(
                csi_products_s3,
                csi_products_bucket,
                path_loc,
                self.rlies1_local_path,
                logger=self.logger)
        self.logger.info('Getting rlies2 products:')
        for path_loc in self.job.rlies2_product_paths_json:
            if path_loc.startswith("/eodata/HRSI/CLMS"):
                path_loc = '/'.join(path_loc.split('/')[3:])
            self.logger.info(' -> %s'%path_loc)
            self.download_prefixed_objects(
                csi_products_s3,
                csi_products_bucket,
                path_loc,
                self.rlies2_local_path,
                logger=self.logger)



    def job_processing(self):
        
        csi_s3, csi_buckets_names = DiasStorageUtil.get_csi_buckets_conf()
        
        #retrieve docker image
        docker_image = SystemPrameters().get(self.logger.debug).docker_image_for_rliepart2_processing

        #RLIE S1S2 processing
        self.output_dir = os.path.join(self.local_job_dir, 'outputs', 'processing')
        os.makedirs(self.output_dir, exist_ok=True)

        cmd = f'rlie_s1s2_processing_chain.py ' \
                f'--rlie_s1_dir {self.to_docker_path(self.rlies1_local_path)} ' \
                f'--rlie_s2_dir {self.to_docker_path(self.rlies2_local_path)} ' \
                f"--date_to_process {self.job.process_date.strftime('%Y-%m-%d')} " \
                f'--tile_id {self.job.tile_id} ' \
                f'--output_dir {self.to_docker_path(self.output_dir)} ' \
                f'--s2tiles_eea39_gdal_info {self.to_docker_path(os.path.join(self.local_job_dir, self.input_products, "static"))}/rlie_s1_static_aux/sentinel2tiles/s2tiles_eea39_gdal_info.json ' \
                f'--temp_dir {self.to_docker_path(os.path.join(self.local_job_dir, "temp"))} ' \
                f'--nprocs 2 ' \
                f'--verbose 2 ' \
                f'--overwrite'
        self.run_processing_in_docker(
            image_name=docker_image,
            container_name=f'rlies1s2_processing_{self.job.id}', #TBD: check how this is necessary
            command=cmd,
            volumes_binding={
                self.local_job_dir: { # The local working dir
                    'bind': self.docker_mount_dir,
                    'mode': 'rw'
                    }
                },
            logger=self.logger
            )
            


    def job_post_processing(self):
        
        csi_s3, csi_buckets_names = DiasStorageUtil.get_csi_buckets_conf()
        csi_products_s3, csi_products_bucket = DiasStorageUtil.get_product_bucket_conf()

        self.logger.info(f'set the completion date for the product')
        self.job.rlies1s2_completion_date = datetime.utcnow()
        
        yaml_product_file = os.path.join(self.output_dir, 'data', 'product_dict.yaml')
        with open(yaml_product_file) as ds:
            product_dict = yaml.load(ds)
            
        self.logger.info(f'upload the products in the HR-S&I products bucket')
        assert len(product_dict) == 1, '%d products generated instead of 1'%len(product_dict)
        value = list(product_dict.values())[0]
        self.job.measurement_date_rlies1s2 = datetime.strptime(os.path.basename(value).split('_')[1], '%Y%m%dT%H%M%S')
        assert self.job.measurement_date_rlies1s2.strftime('%Y%m%d') == self.job.process_date.strftime('%Y%m%d')
        output_dir = 'CLMS/Pan-European/High_Resolution_Layers/Ice/RLIE_S1S2/%s'%self.job.measurement_date_rlies1s2.strftime('%Y/%m/%d')
        input_path = os.path.join(self.output_dir, 'data', value)
        output_path = os.path.join(output_dir, value)
        
        #load json info into job attr rlies1_product_json_submitted_json (to be saved to database at the end of job_post_processing)
        dias_catalog_submit_json = os.path.join(input_path, 'dias_catalog_submit.json')
        with open(dias_catalog_submit_json) as ds:
            self.job.rlies1s2_json_submitted_json = json.load(ds)
        #delete json info file before product upload
        os.unlink(dias_catalog_submit_json)
        

        #copy thumbnail file and delete it
        thumbnail_path_in = os.path.join(input_path, value + '_QLK.png')
        thumbnail_path_out = os.path.join('Preview', output_path, 'thumbnail.png')
        self.logger.info(f'Uploading thumbnail to %s'%(os.path.join(csi_products_bucket, thumbnail_path_out)))
        self.upload_file(
            csi_products_s3,
            csi_products_bucket,
            thumbnail_path_in,
            thumbnail_path_out)
        #delete thumbnail info file before product upload
        os.unlink(thumbnail_path_in)
        

        self.upload_directory(
            csi_products_s3,
            csi_products_bucket,
            input_path,
            output_dir)
        self.logger.info(f'Uploading product to %s'%(os.path.join(csi_products_bucket, output_path)))
        self.job.rlies1s2_path = output_path
        self.job.rlies1s2_json_submitted_json['resto']['properties']['host_base'] = 's3.waw2-1.cloudferro.com'
        self.job.rlies1s2_json_submitted_json['resto']['properties']['productIdentifier'] = os.path.join('/eodata/HRSI', self.job.rlies1s2_path)
        self.job.rlies1s2_json_submitted_json['resto']['properties']['thumbnail'] = thumbnail_path_out
        self.job.rlies1s2_json_submitted_json['resto']['properties']['s3_bucket'] = csi_products_bucket
        
        self.logger.info(f'update some job informations')
        self.job.patch(patch_foreign=True, logger_func=self.logger.debug)




# Function does the minimal stuff to get a valid job, an associate logger and
# launches the 'run' function which actually executes the tasks. This function
# catch errors from 'run' command and "sends" them to the database (with the
# status change).
def main(job_id: int):
    # TODO : Replace this line with the correct job type class
    jobs: List[RlieS1S2Job] = RlieS1S2Job().id_eq(job_id).get()

    if len(jobs) == 0:
        print(f'there is no job which ID is {job_id}')
        exit(1)

    job = jobs[0]
    logger = Logger(job.post_new_execution_info())

    # Create an instance of the RunWorker class to process the job..
    # TODO : Replace this line with the correct run worker class
    run_worker_instance = RunRlieS1S2Worker(job, logger)

    try:
        run_worker_instance.run()
        logger.info('run finished')
        error = None
    except Exception as exception:
        error = exception
        if not isinstance(error, (CsiInternalError, CsiExternalError)):
            logger.error(traceback.format_exc())
    finally:
        run_worker_instance.error_management_and_exit(
            error
        )
# End of mandatory methods
