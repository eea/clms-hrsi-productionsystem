import sys

assert sys.version_info >= (3, 0, 0)

import os
import pathlib
import time
import traceback
import shutil
import socket
import requests
import docker as docker_native

# Check if environment variable is set, exit in error if it's not
from ...common.python.util.sys_util import SysUtil
SysUtil.ensure_env_var_set('COSIMS_DB_HTTP_API_BASE_URL')

from . import docker
from ...common.python.util.s3_util import S3Util
from ...common.python.util.dias_storage_util import DiasStorageUtil
from ...common.python.util.exceptions import CsiInternalError, CsiExternalError
from ...common.python.database.logger import Logger
from ...common.python.database.model.job.job_status import JobStatus
from ...common.python.database.rest.stored_procedure import StoredProcedure



class RunWorker(object):
    '''
    Class of "run_worker" objects in charge of setting up a worker and run 
    dockerized scientific softwares on it to generate products.

    :param root_dir: path leading to the directory in which will be stored all 
        the temporary files needed and/or generated by the processing.
    '''

    def __init__(self, root_dir="/opt/csi"):
        self.root_dir = root_dir
        self.input_products = "NotImplemented"          # Name of the input product used for the processing
        self.job = "NotImplemented"                     # job instance
        self.logger = "NotImplemented"                  # logger instance
        self.local_input_directories = []               # List of directories in which were stored temporary data that need to be cleaned



    def remove_prefix(self, text: str, prefix: str) -> str:
        '''
        Utility function removing a prefix from a string.

        :param text: string from which the prefix should be removed.
        :param prefix: prefix to remove.
        
        '''

        if text.startswith(prefix):
            return text[len(prefix):]
        return text


    def raise_internal_error_if_env_not_set_or_empty(self, env_name: str):
        '''
        Utility function raising an error if a variable is not set in the 
        environment.

        :param env_name: name of the environment variable to retrive.
        '''

        if env_name not in os.environ:
            raise CsiInternalError(
                'Missing env var',
                f'{env_name} environment variable not found')
        if os.environ[env_name] == '':
            raise CsiInternalError(
                'Empty env var',
                f'{env_name} is set but contains an empty string')


    def append_trailing_slash_if_missing(self, string: str):
        '''
        Utility function adding a slash to the end of a sting.
        
        :param string: string to which will be added a trailing slash.
        '''

        return os.path.join(string, '')


    def get_s3(self, endpoint_url: str, access_key: str, secret_key: str):
        '''
        Utility function establishing a connection with, and returning an s3 
        storage object.
        
        :param endpoint_url: s3 storage endpoint url.
        :param access_key: key to access the s3 endpoint storage.
        :param secret_key: secret to access the s3 endpoint storage.
        '''

        s3_resource = S3Util.get_s3(
            endpoint_url,
            access_key,
            secret_key
            )
        return s3_resource
    

    def check_bucket(self, s3_resource, bucket: str):
        '''
        Utility function to check if a bucket exists on an S3 endpoint storage.
        
        :param s3_resource: s3 resource object.
        :param bucket: name of the bucket to test.
        '''

        S3Util.check_bucket(s3_resource, bucket)


    def assert_pre_conditions(self):
        '''
        Utility function to ensure that the required buckets are accessible for 
        the processing :
          - sip_aux
          - infra
          - eodata
          - HRSI (bucket where are stored the generated products)
        '''

        self.raise_internal_error_if_env_not_set_or_empty('CSI_ROOT_DIR')

        self.raise_internal_error_if_env_not_set_or_empty(
            'CSI_INTERNAL_EC2_CREDENTIALS_ACCESS_KEY'
        )
        self.raise_internal_error_if_env_not_set_or_empty(
            'CSI_INTERNAL_EC2_CREDENTIALS_SECRET_KEY'
        )

        self.raise_internal_error_if_env_not_set_or_empty(
            'CSI_PRODUCTS_BUCKET_EC2_CREDENTIALS_ACCESS_KEY'
        )
        self.raise_internal_error_if_env_not_set_or_empty(
            'CSI_PRODUCTS_BUCKET_EC2_CREDENTIALS_SECRET_KEY'
        )

        self.raise_internal_error_if_env_not_set_or_empty('CSI_BUCKET_NAME_SIP_RESULTS')


        csi_s3, csi_buckets_names = DiasStorageUtil.get_csi_buckets_conf()
        self.check_bucket(csi_s3, csi_buckets_names['sip_aux'])
        self.check_bucket(csi_s3, csi_buckets_names['infra'])

        eodata_s3, eodata_bucket = DiasStorageUtil.get_eoadata_bucket_conf()
        self.check_bucket(eodata_s3, eodata_bucket)

        csi_products_s3, csi_products_bucket = DiasStorageUtil.get_product_bucket_conf()
        self.check_bucket(csi_products_s3, csi_products_bucket)


    def file_exists_in_bucket(
        self, s3_resource, bucket: str, object_in_bucket: str
    ):
        '''
        Utility function to check if an object exists on an S3 endpoint storage.
        
        :param s3_resource: s3 resource object.
        :param bucket: name of the bucket in which is stored the object.
        :param object_in_bucket: path leading to the object to test on the bucket.
        '''

        return S3Util.object_exists(
            s3_resource, bucket, object_in_bucket)

            
    def download_file_from_bucket(
        self, s3_resource, bucket: str, object_in_bucket: str, local_file_name: str
    ):
        '''
        Utility function to download a file stored on an S3 endpoint storage, 
        into a specified path on a local worker machine.
        
        :param s3_resource: s3 resource object.
        :param bucket: name of the bucket in the s3 endpoint storage.
        :param object_in_bucket: path leading to the object to download in the bucket.
        :param local_file_name: path under which will be saved the downloaded file.
        '''

        S3Util.download_file_from_bucket(
            s3_resource, bucket, object_in_bucket, local_file_name)


    def download_prefixed_objects(
        self, s3_resource, bucket: str, objects_prefix: str, local_directory_name: str, logger: Logger = None
    ):
        '''
        Utility function to download a set of objects which have a path matching 
        a prefix on an S3 endpoint storage, into a specified folder on a local 
        worker machine.

        :param s3_resource: s3 resource object.
        :param bucket: name of the bucket in the s3 endpoint storage.
        :param objects_prefix: path prefix leading to the objects to download on the bucket.
        :param local_directory_name: path of the directory under which will be saved the downloaded file.
        :param logger: logger object used to display messages.
        '''

        S3Util.download_prefixed_objects(
            s3_resource, bucket, objects_prefix, local_directory_name, logger)


    def compute_local_file_path(
        self, s3_object: str, local_directory_name: str, objects_prefix: str = 'None', logger: Logger = None
    ):
        '''
        Utility function computing the local path on the worker of an object 
        stored on an S3 endpoint storage.

        :param s3_object: path leading to the object to download on the bucket.
        :param local_directory_name: path of the directory under which will be saved the file to download.
        :param objects_prefix: bucket path prefix which won't be added to the "local_directory_name".
        :param logger: logger object used to display messages.
        '''

        return S3Util.compute_local_file_path(
            s3_object, local_directory_name, objects_prefix, logger)


    def upload_directory(
        self, s3_resource, bucket: str, local_directory_name: str, s3_destination: str
    ):
        '''
        Utility function uploading a given directory stored on a local worker 
        machine into a specified location on an S3 endpoint storage.
        
        :param s3_resource: s3 resource object.
        :param bucket: name of the bucket in the s3 endpoint storage.
        :param local_directory_name: path of the directory under which is saved the directory to upload.
        :param s3_destination: path under which will be saved the directory on the bucket.
        '''

        S3Util.upload_directory(
            s3_resource, bucket, local_directory_name, s3_destination)

    def upload_file(
        self, s3_resource, bucket: str, local_file_name: str, s3_destination_filepath: str
    ):
        '''
        Utility function uploading a given directory stored on a local worker 
        machine into a specified location on an S3 endpoint storage.
        
        :param s3_resource: s3 resource object.
        :param bucket: name of the bucket in the s3 endpoint storage.
        :param local_file_name: path of the directory under which is saved the directory to upload.
        :param s3_destination_filepath: s3_destination_filepath: filepath to store file on bucket
        '''

        S3Util.upload_file(
            s3_resource, bucket, local_file_name, s3_destination_filepath)


    def get_work_dir(self) -> str:
        '''
        Utility function rturning the path leading to the working directory.
        This directory will contain a set of subfolders in which will be stored 
        all the temporary files needed and/or generated by the processing.
        '''

        work_dir = os.path.join(self.root_dir, 'work')
        return work_dir


    def get_job_sub_dir(self, job) -> str:
        '''
        Utility function computing the name of a specific job subdirectory,
        providing a specific job object.
        
        :param job: specific job object, which must have a "unique_id" parameter.
        '''

        job_dir = os.path.join('jobs', str(job.unique_id))
        return job_dir


    def get_local_job_dir(self, job) -> str:
        '''
        Utility function returning the path leading to a specific job subdirectory,
        providing a specific job object.

        :param job: specific job object, which must have a "unique_id" parameter.
        '''

        return os.path.join(
            self.get_work_dir(),
            self.get_job_sub_dir(job))


    def get_container_work_dir(self) -> str:
        '''
        Utility function returning the path leading to the container working 
        directory.
        '''

        return '/work'

    
    def get_sip_temp_local_directory(self, job):
        '''
        Utility function returning the path in which will be stored the processing 
        software temporary files.

        :param job: specific job object, which must have a "unique_id" parameter.
        '''

        job_dir = self.get_local_job_dir(job)
        temp_local_directory = os.path.join(job_dir, 'temp')
        return temp_local_directory


    def get_local_results_dir(self, job) -> str:
        '''
        Utility function returning the path in which will be stored the products 
        generated by the processing, providing a specific job object.

        :param job: specific job object, which must have a "unique_id" parameter.
        '''

        job_dir = self.get_local_job_dir(job)
        local_results_dir = os.path.join(job_dir, 'outputs')
        return local_results_dir


    def get_input_product_local_directory(self, job, input_products: str) -> str:
        '''
        Utility function returning the path in which will be stored the input 
        products, downloaded from an external storage, and required for the 
        processing of a specific job.

        :param job: specific job object, which must have a "unique_id" parameter.
        :param input_products: name of the input product which will be stored.
        '''

        job_dir = self.get_local_job_dir(job)
        input_product_local_directory = os.path.join(job_dir, input_products)
        return input_product_local_directory


    def prepare_local_context_for_the_job(self, job, logger: Logger):
        '''
        Utility function creating the required subfolders, for a specific job 
        processing, in the working directory.
        
        :param job: specific job object, which must have a "unique_id" parameter.
        :param logger: logger object used to display messages.
        '''

        job_dir = self.get_local_job_dir(job)
        pathlib.Path(job_dir).mkdir(parents=True, exist_ok=True)
        local_results_dir = self.get_local_results_dir(job)
        pathlib.Path(local_results_dir).mkdir(parents=True, exist_ok=True)
        os.chdir(job_dir)

    
    def run_processing_in_docker(
        self, image_name: str, container_name: str, command: str, volumes_binding: dict, logger: Logger
    ):
        '''
        Utility function running a docker image, providing the container name, 
        the command to execute, and the volumes to bind.
        
        :param image_name: name of the docker image to be run.
        :param container_name: name of the container in which will be run the image.
        :param command: commad to be executed in the container.
        :param volumes_binding: dictionary binding a VM local directory to a container one, 
            so that data stored inside it can be accessed from inside the container.
        :param logger: logger object used to display messages.
        '''

        try:
            return docker.run_processing_in_docker(
                image_name, container_name, command, volumes_binding, logger)

        except requests.exceptions.HTTPError as error:
            message = f"Request HTTP error raised during the software run !"\
                f"\nerror : {error}"
            external_error = CsiExternalError('Request HTTP error', message)
            raise external_error from error

        except requests.exceptions.ConnectionError as error:
            message = f"Request unknown error raised during the software run !"\
                f"\nerror : {error}"
            external_error = CsiExternalError('Request unknown error', message)
            raise external_error from error

        except docker_native.errors.APIError as error:
            message = f"Failed to establish connection with docker to run the software !"\
                f"\nerror : {error}"
            external_error = CsiExternalError('Docker connection error', message)
            raise external_error from error


    def job_pre_processing(self):
        '''
        Functional function designed to download input products, as well as 
        all the other required files, arranging them in adequat folders, to 
        prepare a specific job processing.

        This function can also be used to perform any other operation to 
        prepare the job processing.

        As the function implementation might be specific to the type of job 
        to process, its implementation is left to be done by the team in charge 
        of this job type integration.
        '''

        raise NotImplementedError


    def job_processing(self):
        '''
        Functional function designed to run the docker image, executing a 
        scientific software, to process a specific job and generate products 
        in a local folder of the worker.

        As the function implementation might be specific to the type of job 
        to process, its implementation is left to be done by the team in charge 
        of this job type integration.

        Note that currently for S2 products a "processing_status" is returned 
        by the scientific software execution. This returncode is then passed 
        to the method below, "check_processing_status", to analyze it.
        '''

        raise NotImplementedError


    def job_post_processing(self):
        '''
        Functional function designed to perform post processing actions.
        It can check if products have been generated during the processing, to 
        call the "manage_results()" function if needed, and update parameters 
        in the job that has been processed.

        This function can also be used to perform any other operation to 
        conclude the job processing.

        As the function implementation might be specific to the type of job 
        to process, its implementation is left to be done by the team in charge 
        of this job type integration.
        '''

        raise NotImplementedError


    def run(self):
        '''
        Function that executes the tasks needed to complete a S&I job. It needs a
        valid job object and a logger. This function returns nothing and normally only
        raises two kinds of exceptions: CsiInternalError and CsiExternalError. Any
        other exception is considered a bug.
        '''

        try:
            self.logger.info(f'start the script that manages the job tasks')
            self.logger.info(f'  for job {self.job.id}')
            self.logger.info(
                f'  on worker: name = {socket.gethostname()}, IP = '
                f'{socket.gethostbyname(socket.gethostname())}'
            )
        except socket.gaierror as e:
            raise CsiExternalError(
                subtype="Worker Socket Error",
                message=f"{str(e)}"
            )

        if (isinstance(self.job.last_status_id, int) 
            and self.job.last_status_id == JobStatus.ready.value):
            # If the status is still set to"ready" that means that the job-execution
            # service didn't have time to switch to queued. We wait a bit to let it
            # happen before switching to "started". 5 seconds should be enough.
            time.sleep(5)
            
        self.logger.info(f'set the job status to "started"')
        try:
            self.job.post_new_status_change(JobStatus.started)
        except CsiInternalError as error:
            # This error is due to a new allocation affected by Nomad to a 
            # job that already has one running. This new allocation should 
            # not run to not interfer with the one running, so we stop the 
            # code execution at this point.
            if error.subtype == "Job Status Transition Error":
                return
            else:
                raise error

        self.logger.info(f'assert_pre_conditions')
        self.assert_pre_conditions()
        self.logger.info(f'prepare_local_context_for_the_job')
        self.prepare_local_context_for_the_job(self.job, self.logger)

        self.logger.info(f'prepare the data for the processing, set the job status to "pre_processing"')
        self.job.post_new_status_change(JobStatus.pre_processing)
        
        self.job_pre_processing()

        self.logger.info(f'we can start the processing, set its status to "processing"')
        self.job.post_new_status_change(JobStatus.processing)

        self.logger.info(f'run a docker container for the processing')
        self.job_processing()

        self.logger.info(f'manage processing results, set job status to "post_processing"')
        self.job.post_new_status_change(JobStatus.post_processing)

        self.job_post_processing()

        self.logger.info(f'worker finished with success, set job status as processed')
        self.job.post_new_status_change(JobStatus.processed)

        self.logger.info(f'remove inputs from the local directory')
        for directory in self.local_input_directories:
            if os.path.isdir(directory):
                self.logger.info(f'  remove {directory}')
                shutil.rmtree(directory)


    def error_management_and_exit(self, error):
        '''
        Utility function used to perform error management over the overall run 
        worker processing cycle, clean local directories used for the processing, 
        and exit with the appropriate return code. 
        Two types of error should be raised from the run worker processing : 
        - CsiInternalError : for unrecoverable errors linked with the HR-S&I 
            internal system. Some of these issues are known to be temporary, and 
            thus we attempt to restart the processing when they occur.
        - CsiExternalError : for errors linked with external dependencies, that 
            might be temporary.

        Three types of exitcode can be raised to Nomad: 
        - 0 : by returning 0 we inform Nomad not to launch this processing again.
        - 1 : by returning 1 we inform Nomad that a critical error occurred.
        - 2 : by returning 2 we inform Nomad that it should attempt to run this 
            processing again.

        :param error: exception raised during the run worker processing.
        '''

        error_code = 0

        if error is not None and isinstance(error, CsiInternalError):
            self.logger.error(error.message)
            # If an unpredictable issue happen during the si_software processing,
            #  we attempt to re-run it, until 3 run failed.
            if (error.subtype == "SIP unrecoverable error"):
                # Get the job status history
                status_history = StoredProcedure.job_status_history(
                    [self.job.fk_parent_job_id],
                    logger_func=self.logger.debug
                )

                # Count the number of times the job failed to be processed
                si_processing_internal_error_cpt = 0
                for i in range(len(status_history)):
                    if (
                        status_history[i].status == JobStatus.internal_error
                        and i > 0
                        and status_history[i-1].status == JobStatus.processing
                    ):
                        si_processing_internal_error_cpt += 1

                if si_processing_internal_error_cpt < 2:
                    self.job.post_new_status_change(
                        JobStatus.internal_error,
                        error_subtype=error.subtype,
                        error_message=error.message
                    )
                    # Add delay after setting a new status to avoid status conflict at 
                    # database level when two requests are too close in terms of time range.
                    time.sleep(2)
                    self.job.post_new_status_change(
                        JobStatus.error_checked,
                        error_message="Error automatically checked as an unpredictable "\
                            "issue occurred during si_software processing."
                    )
                    error_code = 2
                else:
                    self.job.post_new_status_change(
                        JobStatus.internal_error,
                        error_subtype=error.subtype,
                        error_message=error.message
                    )
                    # By design we want to exit here with status code = 0. That way we
                    # express that fact that we know that there is an internal error, we
                    # have done as much as we can to recover, we record this internnally
                    # and the error will be there if we launch the processing again with the
                    # same conditions (parmaters and input files). By returning 0 we inform
                    # Nomad not to launch it again.
                    error_code = 0
            else:
                self.job.post_new_status_change(
                    JobStatus.internal_error,
                    error_subtype=error.subtype,
                    error_message=error.message
                )
                # By design we want to exit here with status code = 0. That way we
                # express that fact that we know that there is an internal error, we
                # have done as much as we can to recover, we record this internnally
                # and the error will be there if we launch the processing again with the
                # same conditions (parmaters and input files). By returning 0 we inform
                # Nomad not to launch it again.
                error_code = 0

        elif error is not None and isinstance(error, CsiExternalError):
            self.logger.error(error.message)
            # If an external error occurred during the job processing we attempt
            # to re-process it with a limit of 3 attempt. If the limit is reached, 
            # an internal error is raised, and the job processing is cancelled, for 
            # operators to investigate on the issue.

            # Get the job status history
            status_history = StoredProcedure.job_status_history(
                [self.job.fk_parent_job_id],
                logger_func=self.logger.debug
            )

            # Count the number of times the job failed to be processed
            si_processing_external_error_cpt = 0
            for i in range(len(status_history)):
                if (
                    status_history[i].status == JobStatus.external_error
                    and i > 0
                    and status_history[i-1].status == JobStatus.processing
                ):
                    si_processing_external_error_cpt += 1

            if si_processing_external_error_cpt < 2:
                # Handle status transition in case socket error is raised before 
                # status 'started' is set
                if self.job.last_status_id == JobStatus.queued.value:
                    self.job.post_new_status_change(JobStatus.started)
                self.job.post_new_status_change(
                    JobStatus.external_error,
                    error_subtype=error.subtype,
                    error_message=error.message
                )
                error_code = 2
                # Add delay after setting a new status to avoid status conflict at 
                # database level when two requests are too close in terms of time range.
                time.sleep(2)
            else:
                self.job.post_new_status_change(
                    JobStatus.internal_error,
                    error_subtype=error.subtype,
                    error_message=error.message
                )
                error_code = 0

        elif error is not None:
            self.job.post_new_status_change(
                JobStatus.internal_error, 
                error_subtype='Unknown exception',
                error_message=(
                    f'An unknown exception occured during worker execution. See '
                    f'error log for more information.'
                    )
            )
            # By design we want to exit here with status code = 0. That way we
            # express that fact that we know that there is an internal error, we
            # have done as much as we can to recover, we record this internnally
            # and the error will be there if we launch the processing again with the
            # same conditions (parmaters and input files). By returning 0 we inform
            # Nomad not to launch it again.
            error_code = 0

        # Before exit, upload the working dir and delete it (if it exists).
        try:
            job_dir = self.get_local_job_dir(self.job)
            if os.path.isdir(job_dir):
                self.logger.info(f'upload the working dir for the job ({job_dir}) in the result bucket')

                input_product_local_directory = self.get_input_product_local_directory(
                    self.job, self.input_products)
                if os.path.isdir(input_product_local_directory):
                    self.logger.info(f'  remove input product dir before uploading the working dir')
                    shutil.rmtree(input_product_local_directory)

                sip_temp_local_directory = self.get_sip_temp_local_directory(self.job)
                if os.path.isdir(sip_temp_local_directory):
                    self.logger.info(f'  remove processing temp dir before uploading the working dir')
                    shutil.rmtree(sip_temp_local_directory)

                results_bucket = os.environ['CSI_BUCKET_NAME_SIP_RESULTS']
                csi_s3, _ = DiasStorageUtil.get_csi_buckets_conf()

                # TODO remove the job_dir in bucket before uploading a new one.
                # Maybe this must be done in upload_directory. This can be done
                # with something like: https://stackoverflow.com/a/53836093
                self.upload_directory(
                    csi_s3,
                    results_bucket,
                    job_dir,
                    'work/jobs'
                )
        except Exception as error:
            self.logger.error('an unexpected error occured during the upload of the results dir')
            self.logger.error(traceback.format_exc())
            # We don't change the job status because this error can ocurred
            # after a job has been "completed", and in that case we don't want
            # to interfer with the following workflow.
            #
            # So the only way to see this error is to check the run_worker logs.
        finally:
            job_dir = self.get_local_job_dir(self.job)
            if os.path.isdir(job_dir):
                self.logger.info(f'clean the working dir for the job ({job_dir})')
                shutil.rmtree(job_dir)

        self.logger.info('done.')
        exit(error_code)
